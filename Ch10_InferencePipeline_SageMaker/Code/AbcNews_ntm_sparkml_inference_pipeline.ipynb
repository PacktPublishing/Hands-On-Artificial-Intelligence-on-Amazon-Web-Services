{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Inference Pipeline in SageMaker to Conduct Feature Processing, NTM Training and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Preprocessing](#Preprocessing)   \n",
    "   1. [Create-Features-Through-SparkML-jobs](#Create-Features-Through-SparkML-jobs)\n",
    "3. [Create Training Validation and Test Datasets](#Create-Training-Validation-and-Test-Datasets)\n",
    "   1. [Store Headlines on S3 in Protobuf format](#Store-Headlines-on-S3-in-Protobuf-format)\n",
    "4. [Model Training](#Model-Training)\n",
    "5. [SageMaker Inference Pipeline](#SageMaker-Inference-Pipeline)\n",
    "   1. [Real Time Predictions](#Real-Time-Predictions)\n",
    "   2. [Batch Predictions](#Batch-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Through this notebook, we will demonstrate how SageMaker platform can be used to automate feature processing through Glue, model training, deployment, and inference. Being able to automate these key stages in machine learning life cycle, will enable data scientists and machine learning engineers to relatively quickly create production-ready solutions for business problems.\n",
    "\n",
    "#### We will follow the below process to illustrate key ideas\n",
    "-  Create processed dataset using Amazon Glue ETL service to run SparkML jobs\n",
    "-  Identify topics in the processed dataset via training NTM algorithm\n",
    "-  Create inference pipeline consisting of SparkML and NTM models for real time predictions\n",
    "-  Create inference pipeline consisting of SparkML and NTM models for batch predictions\n",
    "\n",
    "\n",
    "#### About the Dataset\n",
    "To illustrate the concepts, we will use [ABC Millions](https://www.kaggle.com/therohk/million-headlines) Headlines dataset. The dataset contains approximately a million news headlines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features Through SparkML jobs\n",
    "\n",
    "AWS Glue is a serverless ETL service, which can execute PySpark/Spark jobs. We will run SparkML jobs using AWS Glue. We will need to assign the current notebook a role, so it can access the Glue service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the current notebook to access AWS Glue service\n",
    "\n",
    "We will first retrieve the current execution role of the notebook. We will then navigate to [IAM Dashboard](http://console.aws.amazon.com/iam/home) to edit the Role to include AWS Glue specific permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::413491515223:role/service-role/AmazonSageMaker-ExecutionRole-20190822T170423\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "default_bucket = 'ai-in-aws'\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Glue as an trusted entity to this role:\n",
    "\n",
    " On the IAM Dashboard, click on __Roles__ on the left-side nav and search for this Role. Click on the target Role to navigate to **Summary** page. Click on **Trust Relationships** tab to add AWS Glue as an additional trusted entity.\n",
    "\n",
    "Click on **Edit trust relationship** to add the following entry to \"Service\" key:\n",
    "\n",
    "\"glue.amazonaws.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the feature processing script using SparkML\n",
    "\n",
    "We are assuming that the source data is already unzipped and uploaded to your S3 bucket (inference-pipeline/input). We will upload the feature processing script (abcheadlines_processing.py) to s3, so that Glue can run the script as a Pyspark job. \n",
    "\n",
    "\n",
    "The feature processing script conducts the following main functions:\n",
    "- Filter the dataset to include only ~100k abc news headlines\n",
    "- Use SparkML feature transformers to tokenize the headlines, remove stop words, get word & document frequency\n",
    "- The processed data to saved to the designated S3 bucket\n",
    "- The SparkML PipelineModel is serialized using MLeap\n",
    "\n",
    "[MLeap](http://mleap-docs.combust.ml/) is a serialization format and execution engine for machine learning pipelines. It serializes the pipeline to an MLeap bundle, which enables data scientists to take models to wherever they go. It supports Spark, scikit-learn and tensorflow for training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_location = sess.upload_data(path='abcheadlines_processing.py', bucket=default_bucket, key_prefix='inference-pipeline/codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload MLeap Dependencies to S3\n",
    "MLeap related software packages need to be made available to Glue job. Download them from the following aws locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-05 01:19:41--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.192.200\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.192.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 36872 (36K) [application/zip]\n",
      "Saving to: ‘python.zip’\n",
      "\n",
      "python.zip          100%[===================>]  36.01K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-09-05 01:19:42 (507 KB/s) - ‘python.zip’ saved [36872/36872]\n",
      "\n",
      "--2019-09-05 01:19:42--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.192.200\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.192.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17319576 (17M) [application/java-archive]\n",
      "Saving to: ‘mleap_spark_assembly.jar’\n",
      "\n",
      "mleap_spark_assembl 100%[===================>]  16.52M  8.46MB/s    in 2.0s    \n",
      "\n",
      "2019-09-05 01:19:44 (8.46 MB/s) - ‘mleap_spark_assembly.jar’ saved [17319576/17319576]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_dep_location = sess.upload_data(path='python.zip', bucket=default_bucket, key_prefix='inference-pipeline/dependencies/python')\n",
    "jar_dep_location = sess.upload_data(path='mleap_spark_assembly.jar', bucket=default_bucket, key_prefix='inference-pipeline/dependencies/jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designate input/output locations for SparkML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Input location of the data, We uploaded our train.csv file to input key previously\n",
    "s3_input_bucket = default_bucket\n",
    "s3_input_key_prefix = 'inference-pipeline/input'\n",
    "s3_input_fn = 'abcnews-date-text.csv.gz' \n",
    "\n",
    "\n",
    "# Output location of the data. The input data will be split, transformed, and \n",
    "# uploaded to output/train and output/validation\n",
    "s3_output_bucket = default_bucket\n",
    "s3_output_key_prefix = 'inference-pipeline/output/' + timestamp_prefix\n",
    "\n",
    "# the MLeap serialized SparkML model will be uploaded to output/mleap\n",
    "s3_model_bucket = default_bucket\n",
    "s3_model_key_prefix = s3_output_key_prefix + '/mleap'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload ABC News Headlines to S3 bucket**\n",
    "Before uploading .gz version of news headlines, make sure that the .zip version of headlines is available in present working directory on local SageMaker compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from zipfile import ZipFile\n",
    "sr_fn = 'abcnews-date-text'\n",
    "\n",
    "\n",
    "with ZipFile(sr_fn + '.zip', 'r') as zip:\n",
    "    news_data = zip.read(sr_fn + '.csv')\n",
    "    gz_news = gzip.open(sr_fn + '.csv.gz', 'wb')\n",
    "    gz_news.write(news_data)\n",
    "    gz_news.close()\n",
    "    \n",
    "sess.upload_data(path=sr_fn+'.csv.gz', bucket=default_bucket, key_prefix=s3_input_key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Glue API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkml-abcnews-2019-09-05-11-42-09\n"
     ]
    }
   ],
   "source": [
    "boto_session = sess.boto_session\n",
    "s3 = boto_session.resource('s3')\n",
    "\n",
    "glue_client = boto_session.client('glue')\n",
    "job_name = 'sparkml-abcnews-' + timestamp_prefix\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to featurize the ABC Headlines dataset',\n",
    "    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--extra-jars' : jar_dep_location,\n",
    "        '--extra-py-files': python_dep_location\n",
    "    },\n",
    "    AllocatedCapacity=10,\n",
    "    Timeout=60,\n",
    ")\n",
    "glue_job_name = response['Name']\n",
    "\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jr_7a04ae15fef0095eb1ff0ed2f180a0491ab41677c9500e400bcf31c794db6cc1\n"
     ]
    }
   ],
   "source": [
    "job_run_id = glue_client.start_job_run(JobName=job_name,\n",
    "                                       Arguments = {\n",
    "                                        '--S3_INPUT_BUCKET': s3_input_bucket,\n",
    "                                        '--S3_INPUT_KEY_PREFIX': s3_input_key_prefix,\n",
    "                                        '--S3_INPUT_FILENAME': s3_input_fn,  \n",
    "                                        '--S3_OUTPUT_BUCKET': s3_output_bucket,\n",
    "                                        '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n",
    "                                        '--S3_MODEL_BUCKET': s3_model_bucket,\n",
    "                                        '--S3_MODEL_KEY_PREFIX': s3_model_key_prefix\n",
    "                                       })['JobRunId']\n",
    "print(job_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Glue Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the ABC News Headlines dataset (processed) from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Acknowledgements, Copyright Information, and Availability**\n",
    "# Source: https://www.kaggle.com/therohk/million-headlines\n",
    "# Source: SageMaker AWS Labs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import io\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "import sagemaker.amazon.common as smamzc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the processed dataset\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(default_bucket)\n",
    "\n",
    "files = my_bucket.objects.filter(Prefix=s3_output_key_prefix)\n",
    "\n",
    "for f in files:\n",
    "    if '.csv' in f.key:\n",
    "        #print(f.key)\n",
    "        abcnews_df = pd.read_csv(os.path.join('s3://', s3_output_bucket, f.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110365, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcnews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#convert dataframe (dense vector) to compressed sparse row matrix\n",
    "abcnews_csr = csr_matrix(abcnews_df, dtype=np.float32)\n",
    "print(abcnews_csr[:16].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110365"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcnews_csr.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_train = int(0.8 * abcnews_csr.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_data = abcnews_csr[:vol_train, :] \n",
    "test_data = abcnews_csr[vol_train:, :] \n",
    "\n",
    "# further split test set into validation set and test set\n",
    "vol_test = test_data.shape[0]\n",
    "val_data = test_data[:vol_test//2, :]\n",
    "test_data = test_data[vol_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88292, 200) (11037, 200) (11036, 200)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Headlines on S3 in Protobuf format\n",
    "The NTM algorithm, and other built-in SageMaker algorithms, accepts data in CSV or RecordIO Protobuf format. SageMaker algorithms work the best when input data is provided in RecordIO wrapped Protobuf format, an efficient format to encode/serialize structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = os.path.join(s3_input_key_prefix,'train')\n",
    "val_prefix = os.path.join(s3_input_key_prefix, 'val')\n",
    "output_prefix = 'inference-pipeline/output'\n",
    "aux_prefix = os.path.join(s3_input_key_prefix, 'aux')\n",
    "\n",
    "s3loc_train_data = os.path.join('s3://', default_bucket, train_prefix)\n",
    "s3loc_val_data = os.path.join('s3://', default_bucket, val_prefix)\n",
    "s3loc_aux_data = os.path.join('s3://', default_bucket, aux_prefix)\n",
    "output_path = os.path.join('s3://', default_bucket, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pbr(sprse_matrix, bucket, prefix, fname):\n",
    "    # Convert sparse matrix to sparse tensor (record io protobuf) - a format required by NTM algorithm\n",
    "    # pbr - Amazon Record Protobuf format\n",
    "    data_bytes = io.BytesIO()\n",
    "    smamzc.write_spmatrix_to_sparse_tensor(array=sprse_matrix, file=data_bytes, labels=None)\n",
    "    data_bytes.seek(0)\n",
    "\n",
    "    # Upload to s3 location specified by bucket and prefix\n",
    "    file_name = os.path.join(prefix, fname)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(file_name).upload_fileobj(data_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload training and validation data\n",
    "convert_to_pbr(train_data, default_bucket, train_prefix, 'train.pbr')\n",
    "convert_to_pbr(val_data, default_bucket, val_prefix, 'val.pbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the vocabulary file from s3 bucket. Remember that the vocabulary file is created by PySpar\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "files = my_bucket.objects.filter(Prefix=s3_output_key_prefix)\n",
    "\n",
    "for f in files:\n",
    "    if '.txt' in f.key:\n",
    "        s3.Bucket(default_bucket).download_file(f.key, 'vocab.txt')\n",
    "\n",
    "        \n",
    "# s3.Bucket(default_bucket).download_file(os.path.join(s3_output_key_prefix, vocabFN), 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload vocabulary file to auxiliary path\n",
    "vocabFN_location = sess.upload_data(path='vocab.txt', bucket=default_bucket, key_prefix=aux_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "To train NTM in SageMaker, we obtain registry path of training docker image of NTM. Additionally, we create Estimator object from SageMaker Python SDK to provide infrastructure specifications. Then, we set hyperparameters and call fit() method of the estimator created to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm_estmtr_abc = sagemaker.estimator.Estimator(container,\n",
    "                                   role,\n",
    "                                   train_instance_count=1,\n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_path,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set hyperparameters of NTM. You will deep dive into NTM in next chapter, where you will learn about each of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "vocab_size = 200\n",
    "ntm_estmtr_abc.set_hyperparameters(num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=30, epochs=150, num_patience_epochs=3, tolerance=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_input(s3loc_train_data, content_type='application/x-recordio-protobuf')\n",
    "s3_val = s3_input(s3loc_val_data, content_type='application/x-recordio-protobuf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_aux = s3_input(s3loc_aux_data, content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-05 17:11:06 Starting - Starting the training job...............\n",
      "2019-09-05 17:13:15 Starting - Launching requested ML instances......\n",
      "2019-09-05 17:14:18 Starting - Preparing the instances for training......\n",
      "2019-09-05 17:15:24 Downloading - Downloading input data\n",
      "2019-09-05 17:15:24 Training - Downloading the training image...\n",
      "2019-09-05 17:15:54 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'3', u'num_topics': u'5', u'epochs': u'150', u'feature_dim': u'200', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'200', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'3', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'5', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] nvidia-smi took: 0.0251967906952 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:15:57.196] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] vocab.txt\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:15:57 INFO 140644550915904] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 WARNING 140644550915904] 0 out of 200 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 INFO 140644550915904] Vocab embedding shape\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 INFO 140644550915904] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 INFO 140644550915904] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1567703768.158276, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567703768.158247}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:16:08.158] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 10965, \"num_examples\": 1, \"num_bytes\": 1200}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:08 INFO 140644550915904] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:16:39.629] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 31470, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] # Finished training epoch 1 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] Loss (name: value) total: 3.86314327409\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] Loss (name: value) kld: 0.0667051197396\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] Loss (name: value) recons: 3.79643815749\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] Loss (name: value) logppx: 3.86314327409\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:39 INFO 140644550915904] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=3.86314327409\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:16:39.634] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 42437, \"num_examples\": 1, \"num_bytes\": 1100}\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:16:40.026] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 392, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Loss (name: value) total: 3.75588638832\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Loss (name: value) kld: 0.12822487233\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Loss (name: value) recons: 3.62766151809\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Loss (name: value) logppx: 3.75588638832\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] #validation_score (1): 3.755886388323071\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] Timing: train: 31.48s, val: 0.39s, epoch: 31.87s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Total Records Seen\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1567703800.028696, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1567703768.158719}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2770.37053759 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:16:40 INFO 140644550915904] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:17:11.615] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 31586, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] # Finished training epoch 2 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] Loss (name: value) total: 3.66496470869\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] Loss (name: value) kld: 0.212913242172\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] Loss (name: value) recons: 3.45205146748\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] Loss (name: value) logppx: 3.66496470869\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:11 INFO 140644550915904] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=3.66496470869\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:17:12.011] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 394, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Loss (name: value) total: 3.48693231969\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Loss (name: value) kld: 0.264876741025\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Loss (name: value) recons: 3.22205557585\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Loss (name: value) logppx: 3.48693231969\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] #validation_score (2): 3.48693231968962\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] Timing: train: 31.59s, val: 0.40s, epoch: 31.99s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5888, \"sum\": 5888.0, \"min\": 5888}, \"Total Records Seen\": {\"count\": 1, \"max\": 176584, \"sum\": 176584.0, \"min\": 176584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1567703832.016364, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1567703800.028974}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2760.20061628 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:12 INFO 140644550915904] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:17:43.301] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 31284, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] # Finished training epoch 3 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) total: 3.43798265737\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) kld: 0.343147465376\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) recons: 3.09483519207\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) logppx: 3.43798265737\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=3.43798265737\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:17:43.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 401, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) total: 3.26252376789\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) kld: 0.4040288881\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) recons: 2.85849487381\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Loss (name: value) logppx: 3.26252376789\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] #validation_score (3): 3.262523767885352\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] Timing: train: 31.29s, val: 0.41s, epoch: 31.69s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8832, \"sum\": 8832.0, \"min\": 8832}, \"Total Records Seen\": {\"count\": 1, \"max\": 264876, \"sum\": 264876.0, \"min\": 264876}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1567703863.708025, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1567703832.016629}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2785.98151101 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:17:43 INFO 140644550915904] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:18:15.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 31400, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] # Finished training epoch 4 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) total: 3.23972894205\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) kld: 0.449832974156\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) recons: 2.78989596587\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) logppx: 3.23972894205\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=3.23972894205\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:18:15.495] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 385, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) total: 3.13917913168\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) kld: 0.48559167214\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) recons: 2.65358745114\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Loss (name: value) logppx: 3.13917913168\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] #validation_score (4): 3.13917913168371\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] patience losses:[3.755886388323071, 3.48693231968962, 3.262523767885352] min patience loss:3.26252376789 current loss:3.13917913168 absolute loss difference:0.123344636202\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] Timing: train: 31.40s, val: 0.39s, epoch: 31.79s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11776, \"sum\": 11776.0, \"min\": 11776}, \"Total Records Seen\": {\"count\": 1, \"max\": 353168, \"sum\": 353168.0, \"min\": 353168}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1567703895.500153, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1567703863.708309}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2777.17937172 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:15 INFO 140644550915904] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:18:46.579] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 31079, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] # Finished training epoch 5 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] Loss (name: value) total: 3.14480405165\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] Loss (name: value) kld: 0.508496745475\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] Loss (name: value) recons: 2.63630730556\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] Loss (name: value) logppx: 3.14480405165\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:46 INFO 140644550915904] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=3.14480405165\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:18:47.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 490, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Loss (name: value) total: 3.07214784821\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Loss (name: value) kld: 0.539756082124\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Loss (name: value) recons: 2.53239177458\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Loss (name: value) logppx: 3.07214784821\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] #validation_score (5): 3.072147848214159\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] patience losses:[3.48693231968962, 3.262523767885352, 3.13917913168371] min patience loss:3.13917913168 current loss:3.07214784821 absolute loss difference:0.0670312834696\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] Timing: train: 31.08s, val: 0.49s, epoch: 31.57s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14720, \"sum\": 14720.0, \"min\": 14720}, \"Total Records Seen\": {\"count\": 1, \"max\": 441460, \"sum\": 441460.0, \"min\": 441460}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1567703927.075567, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1567703895.500388}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2796.2346959 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:18:47 INFO 140644550915904] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:19:18.194] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 31117, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] # Finished training epoch 6 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) total: 3.08908659263\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) kld: 0.542051850086\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) recons: 2.54703473948\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) logppx: 3.08908659263\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=3.08908659263\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:19:18.595] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 400, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) total: 3.01160380669\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) kld: 0.55696446993\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) recons: 2.45463933676\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Loss (name: value) logppx: 3.01160380669\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] #validation_score (6): 3.0116038066922917\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] patience losses:[3.262523767885352, 3.13917913168371, 3.072147848214159] min patience loss:3.07214784821 current loss:3.01160380669 absolute loss difference:0.0605440415219\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] Timing: train: 31.12s, val: 0.41s, epoch: 31.52s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17664, \"sum\": 17664.0, \"min\": 17664}, \"Total Records Seen\": {\"count\": 1, \"max\": 529752, \"sum\": 529752.0, \"min\": 529752}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1567703958.600702, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1567703927.075829}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2800.69670658 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:18 INFO 140644550915904] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:19:49.683] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 31082, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] # Finished training epoch 7 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] Loss (name: value) total: 3.00301794018\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] Loss (name: value) kld: 0.588049620587\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] Loss (name: value) recons: 2.41496831854\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] Loss (name: value) logppx: 3.00301794018\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:49 INFO 140644550915904] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=3.00301794018\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:19:50.068] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 384, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Loss (name: value) total: 2.93373337104\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Loss (name: value) kld: 0.571545056751\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Loss (name: value) recons: 2.3621883181\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Loss (name: value) logppx: 2.93373337104\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] #validation_score (7): 2.933733371038203\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] patience losses:[3.13917913168371, 3.072147848214159, 3.0116038066922917] min patience loss:3.01160380669 current loss:2.93373337104 absolute loss difference:0.0778704356541\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] Timing: train: 31.08s, val: 0.39s, epoch: 31.47s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20608, \"sum\": 20608.0, \"min\": 20608}, \"Total Records Seen\": {\"count\": 1, \"max\": 618044, \"sum\": 618044.0, \"min\": 618044}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1567703990.072906, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1567703958.600984}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2805.40782238 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:19:50 INFO 140644550915904] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:20:21.036] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 30963, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] # Finished training epoch 8 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) total: 2.94935742595\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) kld: 0.620630033606\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) recons: 2.32872739305\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) logppx: 2.94935742595\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=2.94935742595\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:20:21.434] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 396, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) total: 2.88754662107\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) kld: 0.62276605516\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) recons: 2.26478057258\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Loss (name: value) logppx: 2.88754662107\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] #validation_score (8): 2.8875466210749017\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] patience losses:[3.072147848214159, 3.0116038066922917, 2.933733371038203] min patience loss:2.93373337104 current loss:2.88754662107 absolute loss difference:0.0461867499633\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] Timing: train: 30.96s, val: 0.40s, epoch: 31.37s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 23552, \"sum\": 23552.0, \"min\": 23552}, \"Total Records Seen\": {\"count\": 1, \"max\": 706336, \"sum\": 706336.0, \"min\": 706336}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1567704021.438819, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1567703990.073176}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2814.9130454 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:21 INFO 140644550915904] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:20:52.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 31129, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] # Finished training epoch 9 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) total: 2.92492454873\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) kld: 0.638246499958\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) recons: 2.28667804348\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) logppx: 2.92492454873\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=2.92492454873\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:20:52.963] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 392, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) total: 2.85354285245\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) kld: 0.654330936072\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) recons: 2.19921192044\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Loss (name: value) logppx: 2.85354285245\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] #validation_score (9): 2.853542852445043\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] patience losses:[3.0116038066922917, 2.933733371038203, 2.8875466210749017] min patience loss:2.88754662107 current loss:2.85354285245 absolute loss difference:0.0340037686299\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] Timing: train: 31.13s, val: 0.40s, epoch: 31.53s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26496, \"sum\": 26496.0, \"min\": 26496}, \"Total Records Seen\": {\"count\": 1, \"max\": 794628, \"sum\": 794628.0, \"min\": 794628}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1567704052.968362, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1567704021.439107}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2800.30708995 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:20:52 INFO 140644550915904] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:21:24.015] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 31046, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] # Finished training epoch 10 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) total: 2.90527317619\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) kld: 0.655152503774\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) recons: 2.25012067321\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) logppx: 2.90527317619\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=2.90527317619\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:21:24.402] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 385, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) total: 2.86619608443\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) kld: 0.660985071718\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) recons: 2.20521100951\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Loss (name: value) logppx: 2.86619608443\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] #validation_score (10): 2.866196084433962\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] patience losses:[2.933733371038203, 2.8875466210749017, 2.853542852445043] min patience loss:2.85354285245 current loss:2.86619608443 absolute loss difference:0.0126532319889\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] Timing: train: 31.05s, val: 0.39s, epoch: 31.44s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29440, \"sum\": 29440.0, \"min\": 29440}, \"Total Records Seen\": {\"count\": 1, \"max\": 882920, \"sum\": 882920.0, \"min\": 882920}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1567704084.403946, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1567704052.968643}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2808.67707981 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:24 INFO 140644550915904] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:21:55.702] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 31297, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] # Finished training epoch 11 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] Loss (name: value) total: 2.90203078741\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] Loss (name: value) kld: 0.668701949027\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] Loss (name: value) recons: 2.23332883593\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] Loss (name: value) logppx: 2.90203078741\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:55 INFO 140644550915904] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=2.90203078741\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:21:56.101] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 397, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Loss (name: value) total: 2.85202498579\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Loss (name: value) kld: 0.6882503966\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Loss (name: value) recons: 2.16377458494\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Loss (name: value) logppx: 2.85202498579\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] #validation_score (11): 2.8520249857889532\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] patience losses:[2.8875466210749017, 2.853542852445043, 2.866196084433962] min patience loss:2.85354285245 current loss:2.85202498579 absolute loss difference:0.00151786665609\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] Timing: train: 31.30s, val: 0.40s, epoch: 31.70s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32384, \"sum\": 32384.0, \"min\": 32384}, \"Total Records Seen\": {\"count\": 1, \"max\": 971212, \"sum\": 971212.0, \"min\": 971212}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1567704116.105103, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1567704084.404192}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2785.14636868 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:21:56 INFO 140644550915904] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:22:27.442] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 31336, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] # Finished training epoch 12 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) total: 2.89798055768\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) kld: 0.679112536136\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) recons: 2.21886802303\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) logppx: 2.89798055768\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=2.89798055768\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:22:27.835] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 392, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) total: 2.86536280695\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) kld: 0.665290875309\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) recons: 2.20007192125\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Loss (name: value) logppx: 2.86536280695\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] #validation_score (12): 2.8653628069525086\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] patience losses:[2.853542852445043, 2.866196084433962, 2.8520249857889532] min patience loss:2.85202498579 current loss:2.86536280695 absolute loss difference:0.0133378211636\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] Timing: train: 31.34s, val: 0.39s, epoch: 31.73s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 35328, \"sum\": 35328.0, \"min\": 35328}, \"Total Records Seen\": {\"count\": 1, \"max\": 1059504, \"sum\": 1059504.0, \"min\": 1059504}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1567704147.83641, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1567704116.105355}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2782.49896745 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:27 INFO 140644550915904] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:22:58.911] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 31074, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] # Finished training epoch 13 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] Loss (name: value) total: 2.90159517178\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] Loss (name: value) kld: 0.685571426016\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] Loss (name: value) recons: 2.21602374419\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] Loss (name: value) logppx: 2.90159517178\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:58 INFO 140644550915904] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=2.90159517178\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:22:59.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 391, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Loss (name: value) total: 2.87651048187\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Loss (name: value) kld: 0.676828022159\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Loss (name: value) recons: 2.19968246127\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Loss (name: value) logppx: 2.87651048187\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] #validation_score (13): 2.87651048187339\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] patience losses:[2.866196084433962, 2.8520249857889532, 2.8653628069525086] min patience loss:2.85202498579 current loss:2.87651048187 absolute loss difference:0.0244854960844\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] Timing: train: 31.08s, val: 0.39s, epoch: 31.47s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 38272, \"sum\": 38272.0, \"min\": 38272}, \"Total Records Seen\": {\"count\": 1, \"max\": 1147796, \"sum\": 1147796.0, \"min\": 1147796}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1567704179.304479, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1567704147.83665}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2805.77415766 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:22:59 INFO 140644550915904] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:23:30.432] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 31127, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] # Finished training epoch 14 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) total: 2.90291908679\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) kld: 0.686594601335\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) recons: 2.21632448163\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) logppx: 2.90291908679\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=2.90291908679\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:23:30.825] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 391, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) total: 2.88756356408\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) kld: 0.699893634737\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) recons: 2.18766992536\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Loss (name: value) logppx: 2.88756356408\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] #validation_score (14): 2.8875635640822575\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] patience losses:[2.8520249857889532, 2.8653628069525086, 2.87651048187339] min patience loss:2.85202498579 current loss:2.88756356408 absolute loss difference:0.0355385782933\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] Timing: train: 31.13s, val: 0.39s, epoch: 31.52s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 41216, \"sum\": 41216.0, \"min\": 41216}, \"Total Records Seen\": {\"count\": 1, \"max\": 1236088, \"sum\": 1236088.0, \"min\": 1236088}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1567704210.826872, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1567704179.304727}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2800.94094731 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] \u001b[0m\n",
      "\u001b[31m[09/05/2019 17:23:30 INFO 140644550915904] # Starting training for epoch 15\u001b[0m\n",
      "\n",
      "2019-09-05 17:24:13 Uploading - Uploading generated training model\n",
      "2019-09-05 17:24:13 Completed - Training job completed\n",
      "\u001b[31m[2019-09-05 17:24:01.975] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 31147, \"num_examples\": 2944, \"num_bytes\": 3327136}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] # Finished training epoch 15 on 88292 examples from 2944 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] Loss (name: value) total: 2.90526550265\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] Loss (name: value) kld: 0.692870270468\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] Loss (name: value) recons: 2.21239522977\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] Loss (name: value) logppx: 2.90526550265\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:01 INFO 140644550915904] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=2.90526550265\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:24:02.378] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 402, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) total: 2.8895630801\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) kld: 0.684063493502\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) recons: 2.20549959015\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) logppx: 2.8895630801\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] #validation_score (15): 2.889563080097306\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] patience losses:[2.8653628069525086, 2.87651048187339, 2.8875635640822575] min patience loss:2.86536280695 current loss:2.8895630801 absolute loss difference:0.0242002731448\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Timing: train: 31.15s, val: 0.40s, epoch: 31.55s\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Total Batches Seen\": {\"count\": 1, \"max\": 44160, \"sum\": 44160.0, \"min\": 44160}, \"Total Records Seen\": {\"count\": 1, \"max\": 1324380, \"sum\": 1324380.0, \"min\": 1324380}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 88292, \"sum\": 88292.0, \"min\": 88292}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1567704242.380247, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1567704210.827112}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] #throughput_metric: host=algo-1, train throughput=2798.18946612 records/second\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 WARNING 140644550915904] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[2019-09-05 17:24:02.792] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 399, \"num_examples\": 368, \"num_bytes\": 412084}\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) total: 2.87428242773\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) kld: 0.682198480906\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) recons: 2.19208395106\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss (name: value) logppx: 2.87428242773\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] #quality_metric: host=algo-1, epoch=15, validation total_loss <loss>=2.87428242773\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Loss of server-side model: 2.87428242773\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Best model based on early stopping at epoch 11. Best loss: 2.85202498579\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Topics from epoch:final (num_topics:5) [wetc 0.48, tu 0.85]:\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] [0.40, 0.94] defends decision denies war anti pm warns un bush report iraq calls public australia minister backs wins tas plans chief\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] [0.52, 0.77] clash top win world tour test pakistan back record cup killed title final talks england set australia us still pm\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] [0.45, 0.90] urged indigenous water power take call lead boost final residents get wa act funds england centre fire help plan funding\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] [0.51, 0.72] new record says found strike set win cup south police fire us go pay court plan rise australia bid deal\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] [0.54, 0.93] charged dies murder man charges crash death dead car two woman accident face charge found attack police injured court sydney\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Saved checkpoint to \"/tmp/tmpV5cdIq/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[09/05/2019 17:24:02 INFO 140644550915904] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 485710.51502227783, \"sum\": 485710.51502227783, \"min\": 485710.51502227783}, \"finalize.time\": {\"count\": 1, \"max\": 448.8680362701416, \"sum\": 448.8680362701416, \"min\": 448.8680362701416}, \"initialize.time\": {\"count\": 1, \"max\": 10960.92414855957, \"sum\": 10960.92414855957, \"min\": 10960.92414855957}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.9850006103515625, \"sum\": 2.9850006103515625, \"min\": 2.9850006103515625}, \"setuptime\": {\"count\": 1, \"max\": 38.85221481323242, \"sum\": 38.85221481323242, \"min\": 38.85221481323242}, \"early_stop.time\": {\"count\": 15, \"max\": 494.67992782592773, \"sum\": 6046.731472015381, \"min\": 386.944055557251}, \"update.time\": {\"count\": 15, \"max\": 31987.201929092407, \"sum\": 474215.11125564575, \"min\": 31365.455150604248}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 16, \"max\": 491.3151264190674, \"sum\": 6410.237789154053, \"min\": 385.09202003479004}}, \"EndTime\": 1567704242.833822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567703757.193092}\n",
      "\u001b[0m\n",
      "Billable seconds: 539\n"
     ]
    }
   ],
   "source": [
    "ntm_estmtr_abc.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics extracted, with the confidence range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.40, 0.94] defends decision denies war anti pm warns un bush report iraq calls public australia minister backs wins tas plans chief\n",
    "\n",
    "[0.52, 0.77] clash top win world tour test pakistan back record cup killed title final talks england set australia us still pm\n",
    "\n",
    "[0.45, 0.90] urged indigenous water power take call lead boost final residents get wa act funds england centre fire help plan funding\n",
    "\n",
    "[0.51, 0.72] new record says found strike set win cup south police fire us go pay court plan rise australia bid deal\n",
    "\n",
    "[0.54, 0.93] charged dies murder man charges crash death dead car two woman accident face charge found attack police injured court sydney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2019-09-05-17-11-06-777\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm_estmtr_abc.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Python SDK provides classes, such Model, SparkMLModel, & PipelineModel, to create an inference pipeline that can be used to conduct feature processing and then fit target algorithm to the processed data. Subsequently, the PipelineModel created can be deployed as an endpoint for real time inferences. Additionally, the PipelineModel can also be deployed in batch mode (Batch Transform), to get inferences for a large volume of data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkMLModel requires schema of the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"headline_text\", \"type\": \"string\"}], \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"headline_text\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Time Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get MLeap serialized model\n",
    "#s3_ntm_output_key_prefix = 'sagemaker/inference-pipeline/output'\n",
    "# output_prefix\n",
    "\n",
    "#Get the data location of the trained ntm model\n",
    "modeldataurl = 's3://{}/{}/{}/{}'.format(default_bucket, output_prefix, ntm_estmtr_abc.latest_training_job.job_name, 'output/model.tar.gz')\n",
    "\n",
    "# Make sure s3_model_key_prefix reflects location with the correct timestamp (navigate to s3 bucket to verify)\n",
    "# s3_model_key_prefix = 'inference-pipeline/output/2019-09-05-11-42-09/mleap'\n",
    "sparkml_data = 's3://{}/{}/{}'.format(default_bucket, s3_model_key_prefix, 'model.tar.gz')\n",
    "\n",
    "ntm_model = Model(model_data=modeldataurl, image=container)\n",
    "\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands. We need this for batch transformation\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "\n",
    "\n",
    "model_name = 'inference-pipeline-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, ntm_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the PipelineModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pass json payload\n",
    "Because the output of SparkML model is a dense vector, we will use JSON format (instead of CSV format) to pass input to the pipeline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\":[{\"topic_weights\":[0.5172129869,0.0405323133,0.2246916145,0.1741439849,0.0434190407]}]}'\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"headline_text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "    },\n",
    "    \"data\": [\n",
    "            #[\"murder conviction court of criminal appeal\"]\n",
    "        #[\"is dabiq captured opposition forces\"]\n",
    "        [\"lisa scaffidi public hearing possible over expenses scandal\"]\n",
    "            ]\n",
    "            \n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inference-pipeline-2019-09-06-00-52-01'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sample of headlines for batch scoring - i.e. retrieve topic mixture for each of the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ip_fn = 'abcnews-batch-input.csv'\n",
    "abchl = pd.read_csv(s3_input_fn)\n",
    "abchl['headline_text'][200000:200027].to_csv(batch_ip_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.upload_data(path=batch_ip_fn, bucket=default_bucket, key_prefix='inference-pipeline/batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................!\n"
     ]
    }
   ],
   "source": [
    "#Number of headlines - choose 10-15\n",
    "input_data_path = 's3://{}/{}/{}'.format(default_bucket, 'inference-pipeline/batch', batch_ip_fn)\n",
    "\n",
    "output_data_path = 's3://{}/{}/{}'.format(default_bucket, 'inference-pipeline/batch/abcnews_output', timestamp_prefix)\n",
    "\n",
    "job_name = 'serial-inference-batch-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "#Define the SageMaker PipelineModel Name captured by model_name\n",
    "model_name = 'inference-pipeline-2019-09-06-00-52-01'\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and NTM stages\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "\n",
    "transformer.transform(data = input_data_path,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV, \n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
