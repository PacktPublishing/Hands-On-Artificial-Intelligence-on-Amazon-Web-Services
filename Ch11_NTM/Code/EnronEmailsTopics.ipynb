{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover topics from Enron Emails: A demo of how NTM works \n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preprocessing](#Preprocessing)\n",
    "   1. [Create Bag-of-Words and Vocabulary](#Create-Bag-of-Words-and-Vocabulary)\n",
    "   2. [TF-IDF Term Frequency Inverse Document Frequency](#TF-IDF-Term-Frequency-Inverse-Document-Frequency)\n",
    "3. [Create Training Validation and Test Datasets](#Create-Training-Validation-and-Test-Datasets)\n",
    "   1. [Store Data on S3](#Store-Data-on-S3)\n",
    "   2. [Model Training](#Model-Training)\n",
    "   3. [Set Hyperparameters](#Set-Hyperparameters)\n",
    "4. [Model Hosting and Inference](#Model-Hosting-and-Inference)\n",
    "  1. [Inference with CSV](#Inference-with-CSV)\n",
    "  2. [Creating Word Cloud from Trained Model](#Creating-Word-Cloud-from-Trained-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In this notebook, we will look at discovering topics from enron emails. Neural Topic Model (NTM) from Amazon SageMaker uses neural networks to learn word embeddings or topics. The word embeddings are derived by minimizing loss when building stochastic representation of input documents/emails.\n",
    "\n",
    "First, we build bag of words representation of each email, with each column representing a word and each row representing an email. The values in the matrix are number of times each word is repeated in a given email. We then scale the counts by multiplying them with TF-IDF factor (Term Frequency-Inverse Document Frequency). This factor ensures that words that are specific to a given email and are not repeated frequently across all emails are given higher weight, relative to the words that commonly occur across all emails (for ex: the, so, as, because etc). \n",
    "\n",
    "The prepared bag of words representation of emails is then fed into a neural network whose architecture is defined by the hyperparameters listed below. The network optimizes across several epochs by minimizing loss in building stochastic representation of emails (topic embeddings) and in reconstructing original emails from topic embeddings.\n",
    "\n",
    "The dataset for this notebook is downloaded from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/bag+of+words). It primarily contains list of words (vocabulary) used across all emails and a lookup table detailing number of occurrences of a word in a given email (EmailID WordID Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Acknowledgements, Copyright Information, and Availability**\n",
    "#Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "First, let's run user defined functions used to conduct common operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run bowemails.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag-of-Words and Vocabulary\n",
    "We will only take 10% of emails to have a manageable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_fn = 'data/docword.enron.txt.gz'\n",
    "percent_emails = .10 # get only a x% of emails to avoid memory errors\n",
    "vocab_ip_fn = 'data/vocab.enron.txt'\n",
    "vocab_op_fn = 'data/vocab.txt'\n",
    "\n",
    "#Get bag-of-words from input of enron emails\n",
    "# We will filter emails to reduce data size\n",
    "# Create vocabulary based on the subset of emails that will be sent to training\n",
    "pvt_emails = prepare_bow_vocab(ip_fn, percent_emails, vocab_ip_fn, vocab_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>28090</th>\n",
       "      <th>28091</th>\n",
       "      <th>28092</th>\n",
       "      <th>28093</th>\n",
       "      <th>28095</th>\n",
       "      <th>28096</th>\n",
       "      <th>28097</th>\n",
       "      <th>28098</th>\n",
       "      <th>28100</th>\n",
       "      <th>28101</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 17524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word_ID   1      3      4      8      9      15     16     19     20     \\\n",
       "email_ID                                                                  \n",
       "1             0      0      0      0      0      0      0      0      0   \n",
       "2             0      0      0      0      0      0      0      0      0   \n",
       "3             0      0      0      0      0      0      0      0      0   \n",
       "4             0      0      0      0      0      0      0      0      0   \n",
       "5             0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "word_ID   21     ...    28090  28091  28092  28093  28095  28096  28097  \\\n",
       "email_ID         ...                                                      \n",
       "1             0  ...        0      0      0      0      0      0      0   \n",
       "2             0  ...        0      0      0      0      0      0      0   \n",
       "3             0  ...        0      0      0      0      0      0      0   \n",
       "4             0  ...        0      0      0      0      0      0      0   \n",
       "5             0  ...        0      0      0      0      0      0      0   \n",
       "\n",
       "word_ID   28098  28100  28101  \n",
       "email_ID                       \n",
       "1             0      0      0  \n",
       "2             0      0      0  \n",
       "3             0      0      0  \n",
       "4             0      0      0  \n",
       "5             0      0      0  \n",
       "\n",
       "[5 rows x 17524 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvt_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3986, 17524)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvt_emails.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Term Frequency Inverse Document Frequency\n",
    "We assume that the words that help surface topics are those that are not repeated across all emails but are common within an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_emails = TF_IDF(pvt_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# convert pivoted dataframe to compressed sparse row matrix\n",
    "# compressed sparse row matrix contains row pointer, column index and values\n",
    "sparse_emails = csr_matrix(pvt_emails, dtype=np.float32)\n",
    "print(sparse_emails[:16].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparse_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_train = int(0.8 * sparse_emails.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_data = sparse_emails[:vol_train, :] \n",
    "test_data = sparse_emails[vol_train:, :] \n",
    "\n",
    "# further split test set into validation set and test set\n",
    "vol_test = test_data.shape[0]\n",
    "val_data = test_data[:vol_test//2, :]\n",
    "test_data = test_data[vol_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3188, 17524) (399, 17524) (399, 17524)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data on S3\n",
    "\n",
    "The NTM algorithm, and other built-in SageMaker algorithms, accepts data in CSV or RecordIO Protobuf format. SageMaker algorithms work the best when input data is provided in RecordIO wrapped Protobuf format, an efficient format to encode/serialize structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "# provide your bucket name here\n",
    "#bucket = '<bucket-name>'\n",
    "bucket = 'ai-in-aws'\n",
    "prefix = 'enronemails'\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "aux_prefix = os.path.join(prefix, 'aux')\n",
    "\n",
    "s3loc_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3loc_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "s3loc_aux_data = os.path.join('s3://', bucket, aux_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the training data for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert compressed sparse row matrix to recordio-wrapped-protobuf format\n",
    "# RecordIO is used to efficiently load large datasets (data can be read continuously and stored in a compressed format) \n",
    "\n",
    "convert_to_pbr(train_data, bucket=bucket, prefix=train_prefix, fname_template='emails_train_part{}.pbr', num_parts=3)\n",
    "convert_to_pbr(val_data, bucket=bucket, prefix=val_prefix, fname_template='emails_val_part{}.pbr', num_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "To train NTM in SageMaker, we obtain registry path of training docker image of NTM. Additionally, we create Estimator object from SageMaker Python SDK to provide infrastructure specifications. Then, we set hyperparameters and call fit() method of the estimator created to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm_estmtr = sagemaker.estimator.Estimator(container,\n",
    "                                   role,\n",
    "                                   train_instance_count=2,\n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_path,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Hyperparameters\n",
    "\n",
    "__feature_dim__ - Represents the size of feature vector. It is set to vocabulary size <br>\n",
    "__num_topics__ - Represents number of topics to extract. We can choose a number here and adjust it based on model performance on test set <br>\n",
    "__mini_batch_size__ - Represents number of training examples to process before updating weights <br>\n",
    "__epochs__ - Represents number of backward and forward passes <br>\n",
    "__num_patience_epochs__ Represents maximum number of bad epochs (epochs where loss does not improve) executed before stopping <br>\n",
    "__optimizer__ - We use Adadelta optimization algorithm. Adpative Delta gradient is an enhanced version of Adagrad (Adaptive Gradient), where learning rate decreases based on rolling window of gradient updates vs all past gradient updates<br>\n",
    "__tolerance__ - Represents threshold for change in loss function - the trainining stops early if the change in loss within the last designated number of patience epochs falls below this threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "vocab_size = 17524 # from shape from pivoted emails dataframe\n",
    "ntm_estmtr.set_hyperparameters(num_topics=num_topics, \n",
    "                        feature_dim=vocab_size, \n",
    "                        mini_batch_size=30, \n",
    "                        epochs=150, \n",
    "                        num_patience_epochs=5, \n",
    "                        tolerance=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload vocabulary file to auxiliary folder on S3 bucket -- this is used to identify words associated with latent topics\n",
    "aux_path = s3_aux_data + \"/\"\n",
    "\n",
    "!aws s3 cp $vocab_op_fn $aux_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_input(s3loc_train_data, distribution='ShardedByS3Key', content_type='application/x-recordio-protobuf')\n",
    "\n",
    "s3_val = s3_input(s3loc_val_data, distribution='FullyReplicated', content_type='application/x-recordio-protobuf')\n",
    "\n",
    "s3_aux = s3_input(s3loc_aux_data, distribution='FullyReplicated', content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-02 00:11:12 Starting - Starting the training job...\n",
      "2019-09-02 00:11:16 Starting - Launching requested ML instances......\n",
      "2019-09-02 00:12:17 Starting - Preparing the instances for training......\n",
      "2019-09-02 00:13:20 Downloading - Downloading input data...\n",
      "2019-09-02 00:13:56 Training - Downloading the training image.\n",
      "\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:11 INFO 139681252058944] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:11 INFO 139681252058944] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'3', u'epochs': u'150', u'feature_dim': u'17524', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:11 INFO 139681252058944] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'17524', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'3', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:11 INFO 139681252058944] nvidia-smi took: 0.0251729488373 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc04b043-fca9-4c6c-92d6-d9ee2e9ae03c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-200-219.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/3eedf91a-ef4a-4c86-b892-ec76f5a34f38', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc04b043-fca9-4c6c-92d6-d9ee2e9ae03c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.2.235.87', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-200-219.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/3eedf91a-ef4a-4c86-b892-ec76f5a34f38', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc04b043-fca9-4c6c-92d6-d9ee2e9ae03c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.2.235.87', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-200-219.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/3eedf91a-ef4a-4c86-b892-ec76f5a34f38', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32mProcess 37 is a shell:server.\u001b[0m\n",
      "\u001b[32mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Using default worker.\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:13.474] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Initializing\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] vocab.txt\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:13 INFO 139681252058944] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'3', u'epochs': u'150', u'feature_dim': u'17524', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'17524', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'3', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] nvidia-smi took: 0.0251710414886 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/2ccc9add-e948-4bb7-a98b-a652f31b4e42', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-235-87.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ec6eb4aa-9264-43e7-acec-920d98370228', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/2ccc9add-e948-4bb7-a98b-a652f31b4e42', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.2.235.87', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-235-87.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ec6eb4aa-9264-43e7-acec-920d98370228', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/2ccc9add-e948-4bb7-a98b-a652f31b4e42', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-235-87.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ec6eb4aa-9264-43e7-acec-920d98370228', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/2ccc9add-e948-4bb7-a98b-a652f31b4e42', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.2.235.87', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-235-87.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ec6eb4aa-9264-43e7-acec-920d98370228', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/2ccc9add-e948-4bb7-a98b-a652f31b4e42', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.2.235.87', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2019-09-02-00-11-12-089', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-2-235-87.ec2.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ec6eb4aa-9264-43e7-acec-920d98370228', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:413491515223:training-job/ntm-2019-09-02-00-11-12-089', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 37 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 38 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:16.794] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] vocab.txt\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:16 INFO 140663462225728] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\n",
      "2019-09-02 00:14:26 Training - Training image download completed. Training in progress.\u001b[32m[09/02/2019 00:14:28 WARNING 139681252058944] 1421 out of 17524 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:28 INFO 139681252058944] Vocab embedding shape\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:28 INFO 139681252058944] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:28 INFO 139681252058944] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 WARNING 140663462225728] 1421 out of 17524 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 INFO 140663462225728] Vocab embedding shape\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 INFO 140663462225728] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 INFO 140663462225728] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1567383272.384083, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567383272.384051}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:32.384] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 15603, \"num_examples\": 1, \"num_bytes\": 10036}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:32 INFO 140663462225728] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1567383272.388279, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567383272.388218}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:32.388] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 18917, \"num_examples\": 1, \"num_bytes\": 16680}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:32 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:32 INFO 139681252058944] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:33.879] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1490, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] # Finished training epoch 1 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] Loss (name: value) total: 9.14465288939\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] Loss (name: value) kld: 0.163367636469\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] Loss (name: value) recons: 8.98128522944\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] Loss (name: value) logppx: 9.14465288939\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:33 INFO 139681252058944] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=9.14465288939\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:33.885] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 20410, \"num_examples\": 1, \"num_bytes\": 8240}\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:34.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 185, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Loss (name: value) total: 9.12549391526\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Loss (name: value) kld: 0.474364733085\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Loss (name: value) recons: 8.65112926777\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Loss (name: value) logppx: 9.12549391526\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] #validation_score (1): 9.125493915264423\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] Timing: train: 1.50s, val: 0.19s, epoch: 1.68s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] #progress_metric: host=algo-2, completed 0 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1567383274.07281, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1567383272.388612}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=630.518134674 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:34 INFO 139681252058944] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:35.247] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2863, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] # Finished training epoch 1 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) total: 8.96181197905\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) kld: 0.231302222857\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) recons: 8.73050971949\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) logppx: 8.96181197905\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=8.96181197905\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:35.252] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 18457, \"num_examples\": 1, \"num_bytes\": 8240}\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:35.441] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 189, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) total: 8.81706362993\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) kld: 0.27830201907\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) recons: 8.53876162798\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Loss (name: value) logppx: 8.81706362993\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] #validation_score (1): 8.817063629932893\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] Timing: train: 2.87s, val: 0.19s, epoch: 3.06s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Total Records Seen\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1567383275.444115, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1567383272.384446}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=694.809420545 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:35 INFO 140663462225728] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:35.527] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1453, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] # Finished training epoch 2 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) total: 8.40487983845\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) kld: 0.234661057923\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) recons: 8.17021884918\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) logppx: 8.40487983845\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=8.40487983845\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:35.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 175, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) total: 8.78769812951\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) kld: 0.219836035753\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) recons: 8.56786209498\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Loss (name: value) logppx: 8.78769812951\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] #validation_score (2): 8.787698129507211\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] Timing: train: 1.45s, val: 0.18s, epoch: 1.64s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1567383275.708439, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1567383274.073091}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=649.341117621 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:35 INFO 139681252058944] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:37.174] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1465, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] # Finished training epoch 3 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) total: 8.27470903043\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) kld: 0.1923834357\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) recons: 8.08232561041\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) logppx: 8.27470903043\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=8.27470903043\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:37.363] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 187, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) total: 8.77768597725\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) kld: 0.226712055695\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) recons: 8.55097382374\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Loss (name: value) logppx: 8.77768597725\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] #validation_score (3): 8.777685977251101\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] Timing: train: 1.47s, val: 0.19s, epoch: 1.66s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 3186, \"sum\": 3186.0, \"min\": 3186}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1567383277.371031, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1567383275.708743}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=638.78392921 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:37 INFO 139681252058944] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:38.394] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 2950, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] # Finished training epoch 2 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) total: 8.43793739713\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) kld: 0.208851617379\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) recons: 8.22908577359\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) logppx: 8.43793739713\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=8.43793739713\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:38.661] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 263, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) total: 8.69483795166\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) kld: 0.181597252381\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) recons: 8.51324079465\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Loss (name: value) logppx: 8.69483795166\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] #validation_score (2): 8.694837951660157\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] Timing: train: 2.95s, val: 0.27s, epoch: 3.22s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}, \"Total Records Seen\": {\"count\": 1, \"max\": 4252, \"sum\": 4252.0, \"min\": 4252}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1567383278.667499, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1567383275.444399}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=659.578004831 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:38 INFO 140663462225728] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:38.895] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1523, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] # Finished training epoch 4 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] Loss (name: value) total: 8.20936378903\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] Loss (name: value) kld: 0.177480192096\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] Loss (name: value) recons: 8.03188360002\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] Loss (name: value) logppx: 8.20936378903\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:38 INFO 139681252058944] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=8.20936378903\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:39.170] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 273, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Loss (name: value) total: 8.72319468963\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Loss (name: value) kld: 0.220591631914\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Loss (name: value) recons: 8.50260314941\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Loss (name: value) logppx: 8.72319468963\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] #validation_score (4): 8.723194689628405\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] Timing: train: 1.53s, val: 0.28s, epoch: 1.80s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 4248, \"sum\": 4248.0, \"min\": 4248}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1567383279.174601, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1567383277.371473}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=588.931089384 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:39 INFO 139681252058944] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:40.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1708, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] # Finished training epoch 5 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] Loss (name: value) total: 8.18236912621\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] Loss (name: value) kld: 0.16315130172\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] Loss (name: value) recons: 8.01921779491\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] Loss (name: value) logppx: 8.18236912621\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:40 INFO 139681252058944] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=8.18236912621\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:41.070] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 184, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Loss (name: value) total: 8.73083746494\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Loss (name: value) kld: 0.16945746862\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Loss (name: value) recons: 8.56137988751\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Loss (name: value) logppx: 8.73083746494\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] #validation_score (5): 8.73083746494391\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] Timing: train: 1.71s, val: 0.19s, epoch: 1.90s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 5310, \"sum\": 5310.0, \"min\": 5310}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1567383281.071249, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1567383279.174831}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=559.963811135 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:41 INFO 139681252058944] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:41.597] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2926, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] # Finished training epoch 3 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) total: 8.35388417848\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) kld: 0.178266148276\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) recons: 8.17561802394\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) logppx: 8.35388417848\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=8.35388417848\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:41.775] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 175, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) total: 8.6716899774\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) kld: 0.170648267942\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) recons: 8.50104162754\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Loss (name: value) logppx: 8.6716899774\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] #validation_score (3): 8.671689977401343\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] Timing: train: 2.93s, val: 0.18s, epoch: 3.11s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 213, \"sum\": 213.0, \"min\": 213}, \"Total Records Seen\": {\"count\": 1, \"max\": 6378, \"sum\": 6378.0, \"min\": 6378}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1567383281.780125, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1567383278.667804}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=683.061637396 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:41 INFO 140663462225728] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:42.493] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1421, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] # Finished training epoch 6 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) total: 8.15491701762\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) kld: 0.153794376055\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) recons: 8.00112265128\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) logppx: 8.15491701762\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=8.15491701762\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:42.733] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 238, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) total: 8.68764409774\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) kld: 0.174483909362\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) recons: 8.51316011869\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Loss (name: value) logppx: 8.68764409774\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] #validation_score (6): 8.68764409774389\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] patience losses:[9.125493915264423, 8.787698129507211, 8.777685977251101, 8.723194689628405, 8.73083746494391] min patience loss:8.72319468963 current loss:8.68764409774 absolute loss difference:0.0355505918845\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] Timing: train: 1.42s, val: 0.24s, epoch: 1.67s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 6372, \"sum\": 6372.0, \"min\": 6372}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1567383282.737321, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1567383281.071495}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=637.470624207 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:42 INFO 139681252058944] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:44.146] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1408, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] # Finished training epoch 7 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) total: 8.13748081349\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) kld: 0.146984878293\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) recons: 7.99049592195\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) logppx: 8.13748081349\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=8.13748081349\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:44.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 2865, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] # Finished training epoch 4 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) total: 8.3218783598\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) kld: 0.16436267011\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) recons: 8.15751569148\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) logppx: 8.3218783598\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=8.3218783598\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:44.831] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 183, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) total: 8.6594136556\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) kld: 0.17537240493\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) recons: 8.48404114552\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Loss (name: value) logppx: 8.6594136556\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] #validation_score (4): 8.659413655598959\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] Timing: train: 2.87s, val: 0.19s, epoch: 3.06s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 284, \"sum\": 284.0, \"min\": 284}, \"Total Records Seen\": {\"count\": 1, \"max\": 8504, \"sum\": 8504.0, \"min\": 8504}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1567383284.836917, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1567383281.780392}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=695.523811329 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:44 INFO 140663462225728] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:44.322] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 174, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) total: 8.72029567621\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) kld: 0.156750489504\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) recons: 8.56354522705\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Loss (name: value) logppx: 8.72029567621\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] #validation_score (7): 8.72029567620693\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] patience losses:[8.787698129507211, 8.777685977251101, 8.723194689628405, 8.73083746494391, 8.68764409774389] min patience loss:8.68764409774 current loss:8.72029567621 absolute loss difference:0.032651578463\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] Timing: train: 1.41s, val: 0.18s, epoch: 1.59s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 7434, \"sum\": 7434.0, \"min\": 7434}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1567383284.324775, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1567383282.737577}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=669.054412189 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:44 INFO 139681252058944] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:45.731] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1405, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] # Finished training epoch 8 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) total: 8.13188253332\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) kld: 0.149969044217\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) recons: 7.98191349595\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) logppx: 8.13188253332\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=8.13188253332\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:45.904] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 172, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) total: 8.69418334961\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) kld: 0.168461372913\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) recons: 8.5257218581\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Loss (name: value) logppx: 8.69418334961\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] #validation_score (8): 8.694183349609375\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] patience losses:[8.777685977251101, 8.723194689628405, 8.73083746494391, 8.68764409774389, 8.72029567620693] min patience loss:8.68764409774 current loss:8.69418334961 absolute loss difference:0.00653925186548\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] Timing: train: 1.41s, val: 0.17s, epoch: 1.58s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 8496, \"sum\": 8496.0, \"min\": 8496}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1567383285.905921, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1567383284.325015}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=671.70504006 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:45 INFO 139681252058944] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:47.685] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2847, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] # Finished training epoch 5 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) total: 8.3115790497\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) kld: 0.162321818602\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) recons: 8.14925724083\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) logppx: 8.3115790497\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=8.3115790497\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:47.868] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 182, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) total: 8.66395854461\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) kld: 0.157030120874\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) recons: 8.50692842924\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Loss (name: value) logppx: 8.66395854461\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] #validation_score (5): 8.663958544608874\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] Timing: train: 2.85s, val: 0.18s, epoch: 3.03s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 355, \"sum\": 355.0, \"min\": 355}, \"Total Records Seen\": {\"count\": 1, \"max\": 10630, \"sum\": 10630.0, \"min\": 10630}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1567383287.869912, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1567383284.83721}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=700.992464152 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:47 INFO 140663462225728] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:47.389] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1480, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] # Finished training epoch 9 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) total: 8.11916117492\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) kld: 0.14672069682\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) recons: 7.97244040878\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) logppx: 8.11916117492\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=8.11916117492\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:47.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 187, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) total: 8.70300394694\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) kld: 0.14391880769\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) recons: 8.55908520038\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Loss (name: value) logppx: 8.70300394694\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] #validation_score (9): 8.703003946940104\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] patience losses:[8.723194689628405, 8.73083746494391, 8.68764409774389, 8.72029567620693, 8.694183349609375] min patience loss:8.68764409774 current loss:8.70300394694 absolute loss difference:0.0153598491962\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] Timing: train: 1.48s, val: 0.19s, epoch: 1.67s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 9558, \"sum\": 9558.0, \"min\": 9558}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1567383287.578883, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1567383285.906221}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=634.864401137 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:47 INFO 139681252058944] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:49.029] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1448, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] # Finished training epoch 10 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) total: 8.11558515054\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) kld: 0.145783608048\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) recons: 7.96980158488\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) logppx: 8.11558515054\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=8.11558515054\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:49.205] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 175, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) total: 8.67278857109\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) kld: 0.144165844795\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) recons: 8.52862282777\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Loss (name: value) logppx: 8.67278857109\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] #validation_score (10): 8.672788571088741\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] patience losses:[8.73083746494391, 8.68764409774389, 8.72029567620693, 8.694183349609375, 8.703003946940104] min patience loss:8.68764409774 current loss:8.67278857109 absolute loss difference:0.0148555266551\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] Timing: train: 1.45s, val: 0.18s, epoch: 1.63s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 10620, \"sum\": 10620.0, \"min\": 10620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1567383289.210624, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1567383287.579111}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=650.876794958 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:49 INFO 139681252058944] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:50.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 2853, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] # Finished training epoch 6 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) total: 8.29862252535\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) kld: 0.155482890125\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) recons: 8.1431396556\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) logppx: 8.29862252535\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=8.29862252535\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:50.893] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 167, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) total: 8.64481220734\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) kld: 0.1418171687\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) recons: 8.50299506554\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Loss (name: value) logppx: 8.64481220734\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] #validation_score (6): 8.64481220734425\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] patience losses:[8.817063629932893, 8.694837951660157, 8.671689977401343, 8.659413655598959, 8.663958544608874] min patience loss:8.6594136556 current loss:8.64481220734 absolute loss difference:0.0146014482547\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] Timing: train: 2.86s, val: 0.17s, epoch: 3.03s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 426, \"sum\": 426.0, \"min\": 426}, \"Total Records Seen\": {\"count\": 1, \"max\": 12756, \"sum\": 12756.0, \"min\": 12756}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1567383290.899049, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1567383287.870154}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=701.87091562 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:50 INFO 140663462225728] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:50.632] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1421, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] # Finished training epoch 11 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) total: 8.0958206318\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) kld: 0.137733407815\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) recons: 7.9580871794\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) logppx: 8.0958206318\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=8.0958206318\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:50.817] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 183, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) total: 8.68264238406\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) kld: 0.151154748599\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) recons: 8.53148780236\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Loss (name: value) logppx: 8.68264238406\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] #validation_score (11): 8.682642384064502\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] patience losses:[8.68764409774389, 8.72029567620693, 8.694183349609375, 8.703003946940104, 8.672788571088741] min patience loss:8.67278857109 current loss:8.68264238406 absolute loss difference:0.00985381297576\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] Timing: train: 1.42s, val: 0.18s, epoch: 1.61s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 11682, \"sum\": 11682.0, \"min\": 11682}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1567383290.81889, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1567383289.210908}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=660.40015674 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:50 INFO 139681252058944] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:52.193] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1373, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] # Finished training epoch 12 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) total: 8.1056054857\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) kld: 0.139370362405\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) recons: 7.96623516083\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) logppx: 8.1056054857\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=8.1056054857\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:52.387] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 192, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) total: 8.68964753762\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) kld: 0.169965429795\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) recons: 8.51968223376\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Loss (name: value) logppx: 8.68964753762\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] #validation_score (12): 8.689647537622696\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] patience losses:[8.72029567620693, 8.694183349609375, 8.703003946940104, 8.672788571088741, 8.682642384064502] min patience loss:8.67278857109 current loss:8.68964753762 absolute loss difference:0.016858966534\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] Timing: train: 1.38s, val: 0.19s, epoch: 1.57s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 12744, \"sum\": 12744.0, \"min\": 12744}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1567383292.388582, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1567383290.819128}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=676.603300938 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:52 INFO 139681252058944] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:53.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2858, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] # Finished training epoch 7 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) total: 8.28243489154\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) kld: 0.152316318319\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) recons: 8.13011860736\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) logppx: 8.28243489154\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=8.28243489154\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:53.929] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1539, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] # Finished training epoch 13 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] Loss (name: value) total: 8.08758947584\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] Loss (name: value) kld: 0.13898257282\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] Loss (name: value) recons: 7.94860691494\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] Loss (name: value) logppx: 8.08758947584\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:53 INFO 139681252058944] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=8.08758947584\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:54.114] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 182, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Loss (name: value) total: 8.64535264235\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Loss (name: value) kld: 0.148453092575\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Loss (name: value) recons: 8.49689957056\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Loss (name: value) logppx: 8.64535264235\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] #validation_score (13): 8.645352642352764\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] patience losses:[8.694183349609375, 8.703003946940104, 8.672788571088741, 8.682642384064502, 8.689647537622696] min patience loss:8.67278857109 current loss:8.64535264235 absolute loss difference:0.027435928736\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] Timing: train: 1.54s, val: 0.19s, epoch: 1.73s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 13806, \"sum\": 13806.0, \"min\": 13806}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1567383294.119283, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1567383292.388883}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=613.681603691 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:54 INFO 139681252058944] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:53.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 199, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) total: 8.65692299085\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) kld: 0.165617858447\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) recons: 8.49130507249\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Loss (name: value) logppx: 8.65692299085\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] #validation_score (7): 8.656922990847857\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] patience losses:[8.694837951660157, 8.671689977401343, 8.659413655598959, 8.663958544608874, 8.64481220734425] min patience loss:8.64481220734 current loss:8.65692299085 absolute loss difference:0.0121107835036\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] Timing: train: 2.86s, val: 0.20s, epoch: 3.06s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 497, \"sum\": 497.0, \"min\": 497}, \"Total Records Seen\": {\"count\": 1, \"max\": 14882, \"sum\": 14882.0, \"min\": 14882}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1567383293.960083, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1567383290.89935}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=694.572644225 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:53 INFO 140663462225728] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:55.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1452, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] # Finished training epoch 14 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) total: 8.09187600171\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) kld: 0.141189226839\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) recons: 7.95068675854\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) logppx: 8.09187600171\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=8.09187600171\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:55.774] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 201, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) total: 8.68740954277\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) kld: 0.158504653588\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) recons: 8.52890480237\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Loss (name: value) logppx: 8.68740954277\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] #validation_score (14): 8.68740954276843\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] patience losses:[8.703003946940104, 8.672788571088741, 8.682642384064502, 8.689647537622696, 8.645352642352764] min patience loss:8.64535264235 current loss:8.68740954277 absolute loss difference:0.0420569004157\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] Timing: train: 1.45s, val: 0.20s, epoch: 1.66s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 14868, \"sum\": 14868.0, \"min\": 14868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1567383295.775922, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1567383294.119654}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=641.146725681 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:55 INFO 139681252058944] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:56.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 2839, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] # Finished training epoch 8 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) total: 8.27578721024\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) kld: 0.152406444012\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) recons: 8.12338077026\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) logppx: 8.27578721024\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=8.27578721024\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:57.244] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1467, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] # Finished training epoch 15 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) total: 8.07218545984\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) kld: 0.133373810627\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) recons: 7.93881166953\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) logppx: 8.07218545984\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=8.07218545984\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:56.992] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 191, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) total: 8.64143598508\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) kld: 0.154525171182\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) recons: 8.48691077599\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Loss (name: value) logppx: 8.64143598508\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] #validation_score (8): 8.641435985076122\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] patience losses:[8.671689977401343, 8.659413655598959, 8.663958544608874, 8.64481220734425, 8.656922990847857] min patience loss:8.64481220734 current loss:8.64143598508 absolute loss difference:0.00337622226813\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] Timing: train: 2.84s, val: 0.20s, epoch: 3.04s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 568, \"sum\": 568.0, \"min\": 568}, \"Total Records Seen\": {\"count\": 1, \"max\": 17008, \"sum\": 17008.0, \"min\": 17008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1567383296.997009, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1567383293.960351}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=700.080151103 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:56 INFO 140663462225728] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:57.431] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 183, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) total: 8.65246718969\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) kld: 0.158816715387\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) recons: 8.49365046575\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Loss (name: value) logppx: 8.65246718969\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] #validation_score (15): 8.652467189691006\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] patience losses:[8.672788571088741, 8.682642384064502, 8.689647537622696, 8.645352642352764, 8.68740954276843] min patience loss:8.64535264235 current loss:8.65246718969 absolute loss difference:0.00711454733824\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] Timing: train: 1.47s, val: 0.19s, epoch: 1.66s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 15930, \"sum\": 15930.0, \"min\": 15930}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1567383297.432985, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1567383295.776201}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=640.939336431 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:57 INFO 139681252058944] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:14:59.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2885, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] # Finished training epoch 9 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] Loss (name: value) total: 8.25724566822\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] Loss (name: value) kld: 0.147958464186\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] Loss (name: value) recons: 8.10928720107\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] Loss (name: value) logppx: 8.25724566822\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:14:59 INFO 140663462225728] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=8.25724566822\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:00.066] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 180, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Loss (name: value) total: 8.61506981483\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Loss (name: value) kld: 0.137654554538\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Loss (name: value) recons: 8.47741542718\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Loss (name: value) logppx: 8.61506981483\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] #validation_score (9): 8.615069814828725\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] patience losses:[8.659413655598959, 8.663958544608874, 8.64481220734425, 8.656922990847857, 8.641435985076122] min patience loss:8.64143598508 current loss:8.61506981483 absolute loss difference:0.0263661702474\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] Timing: train: 2.89s, val: 0.19s, epoch: 3.08s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 639, \"sum\": 639.0, \"min\": 639}, \"Total Records Seen\": {\"count\": 1, \"max\": 19134, \"sum\": 19134.0, \"min\": 19134}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1567383300.07275, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1567383296.997281}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=691.245220576 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:00 INFO 140663462225728] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:58.936] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1502, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] # Finished training epoch 16 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] Loss (name: value) total: 8.08127212524\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] Loss (name: value) kld: 0.131897017029\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] Loss (name: value) recons: 7.94937513846\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] Loss (name: value) logppx: 8.08127212524\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:58 INFO 139681252058944] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=8.08127212524\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:14:59.119] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 180, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Loss (name: value) total: 8.69839274089\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Loss (name: value) kld: 0.167783964597\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Loss (name: value) recons: 8.53060877873\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Loss (name: value) logppx: 8.69839274089\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] #validation_score (16): 8.698392740885417\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] patience losses:[8.682642384064502, 8.689647537622696, 8.645352642352764, 8.68740954276843, 8.652467189691006] min patience loss:8.64535264235 current loss:8.69839274089 absolute loss difference:0.0530400985327\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] Timing: train: 1.50s, val: 0.18s, epoch: 1.69s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 16992, \"sum\": 16992.0, \"min\": 16992}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1567383299.12054, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1567383297.43329}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=629.367300854 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:14:59 INFO 139681252058944] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:00.562] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1439, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] # Finished training epoch 17 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) total: 8.06922639917\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) kld: 0.135443028697\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) recons: 7.93378336871\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) logppx: 8.06922639917\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=8.06922639917\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:00.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 172, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) total: 8.63551033216\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) kld: 0.12505065111\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) recons: 8.51045954778\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Loss (name: value) logppx: 8.63551033216\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] #validation_score (17): 8.63551033215645\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] patience losses:[8.689647537622696, 8.645352642352764, 8.68740954276843, 8.652467189691006, 8.698392740885417] min patience loss:8.64535264235 current loss:8.63551033216 absolute loss difference:0.00984231019631\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] Timing: train: 1.44s, val: 0.18s, epoch: 1.62s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 18054, \"sum\": 18054.0, \"min\": 18054}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1567383300.740666, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1567383299.120823}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=655.555987784 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:00 INFO 139681252058944] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:02.193] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1452, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] # Finished training epoch 18 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) total: 8.07206584789\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) kld: 0.131961667538\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) recons: 7.94010420905\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) logppx: 8.07206584789\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=8.07206584789\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:02.372] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 177, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) total: 8.67244759584\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) kld: 0.140322876588\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) recons: 8.53212468074\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Loss (name: value) logppx: 8.67244759584\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] #validation_score (18): 8.672447595840845\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] patience losses:[8.645352642352764, 8.68740954276843, 8.652467189691006, 8.698392740885417, 8.63551033215645] min patience loss:8.63551033216 current loss:8.67244759584 absolute loss difference:0.0369372636844\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] Timing: train: 1.45s, val: 0.18s, epoch: 1.63s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 19116, \"sum\": 19116.0, \"min\": 19116}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1567383302.373529, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1567383300.74096}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=650.451943507 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:02 INFO 139681252058944] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:02.977] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 2903, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] # Finished training epoch 10 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] Loss (name: value) total: 8.24944156414\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] Loss (name: value) kld: 0.147760373326\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] Loss (name: value) recons: 8.10168121768\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] Loss (name: value) logppx: 8.24944156414\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:02 INFO 140663462225728] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=8.24944156414\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:03.180] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 202, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Loss (name: value) total: 8.62753835825\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Loss (name: value) kld: 0.141054960398\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Loss (name: value) recons: 8.4864833734\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Loss (name: value) logppx: 8.62753835825\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] #validation_score (10): 8.627538358248197\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] patience losses:[8.663958544608874, 8.64481220734425, 8.656922990847857, 8.641435985076122, 8.615069814828725] min patience loss:8.61506981483 current loss:8.62753835825 absolute loss difference:0.0124685434195\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] Timing: train: 2.91s, val: 0.20s, epoch: 3.11s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 710, \"sum\": 710.0, \"min\": 710}, \"Total Records Seen\": {\"count\": 1, \"max\": 21260, \"sum\": 21260.0, \"min\": 21260}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1567383303.181904, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1567383300.073014}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=683.820489775 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:03 INFO 140663462225728] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:03.829] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1455, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] # Finished training epoch 19 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] Loss (name: value) total: 8.05716454541\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] Loss (name: value) kld: 0.132376241022\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] Loss (name: value) recons: 7.92478829843\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] Loss (name: value) logppx: 8.05716454541\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:03 INFO 139681252058944] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=8.05716454541\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:04.044] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 213, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Loss (name: value) total: 8.63745661026\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Loss (name: value) kld: 0.119154373193\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Loss (name: value) recons: 8.51830221323\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Loss (name: value) logppx: 8.63745661026\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] #validation_score (19): 8.637456610263921\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] patience losses:[8.68740954276843, 8.652467189691006, 8.698392740885417, 8.63551033215645, 8.672447595840845] min patience loss:8.63551033216 current loss:8.63745661026 absolute loss difference:0.00194627810747\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] Timing: train: 1.46s, val: 0.21s, epoch: 1.67s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 20178, \"sum\": 20178.0, \"min\": 20178}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1567383304.045471, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1567383302.373771}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=635.228269567 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:04 INFO 139681252058944] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:05.485] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1439, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] # Finished training epoch 20 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) total: 8.06113635169\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) kld: 0.133851536336\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) recons: 7.92728481999\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) logppx: 8.06113635169\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=8.06113635169\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:05.664] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 176, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) total: 8.69089136368\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) kld: 0.146508286549\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) recons: 8.54438292675\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Loss (name: value) logppx: 8.69089136368\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] #validation_score (20): 8.69089136368189\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] patience losses:[8.652467189691006, 8.698392740885417, 8.63551033215645, 8.672447595840845, 8.637456610263921] min patience loss:8.63551033216 current loss:8.69089136368 absolute loss difference:0.0553810315254\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] Timing: train: 1.44s, val: 0.18s, epoch: 1.62s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 21240, \"sum\": 21240.0, \"min\": 21240}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1567383305.666202, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1567383304.045751}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=655.312467478 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:05 INFO 139681252058944] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:05.993] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2810, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] # Finished training epoch 11 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] Loss (name: value) total: 8.24832568101\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] Loss (name: value) kld: 0.14847734008\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] Loss (name: value) recons: 8.09984830758\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] Loss (name: value) logppx: 8.24832568101\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:05 INFO 140663462225728] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=8.24832568101\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:06.175] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 181, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Loss (name: value) total: 8.6139833108\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Loss (name: value) kld: 0.119566723017\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Loss (name: value) recons: 8.49441680908\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Loss (name: value) logppx: 8.6139833108\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] #validation_score (11): 8.613983310797275\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] patience losses:[8.64481220734425, 8.656922990847857, 8.641435985076122, 8.615069814828725, 8.627538358248197] min patience loss:8.61506981483 current loss:8.6139833108 absolute loss difference:0.00108650403145\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] Timing: train: 2.81s, val: 0.19s, epoch: 3.00s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 781, \"sum\": 781.0, \"min\": 781}, \"Total Records Seen\": {\"count\": 1, \"max\": 23386, \"sum\": 23386.0, \"min\": 23386}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1567383306.179909, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1567383303.182149}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=709.160484327 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:06 INFO 140663462225728] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:07.072] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1405, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] # Finished training epoch 21 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) total: 8.05652913694\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) kld: 0.134050564192\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) recons: 7.92247856281\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) logppx: 8.05652913694\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=8.05652913694\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:07.253] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 179, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) total: 8.64257323436\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) kld: 0.135907191497\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) recons: 8.50666605632\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Loss (name: value) logppx: 8.64257323436\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] #validation_score (21): 8.64257323436248\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] patience losses:[8.698392740885417, 8.63551033215645, 8.672447595840845, 8.637456610263921, 8.69089136368189] min patience loss:8.63551033216 current loss:8.64257323436 absolute loss difference:0.00706290220603\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] Timing: train: 1.41s, val: 0.18s, epoch: 1.59s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 22302, \"sum\": 22302.0, \"min\": 22302}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1567383307.254978, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1567383305.666532}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=668.509778549 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:07 INFO 139681252058944] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:08.752] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1497, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] # Finished training epoch 22 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) total: 8.04859472911\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) kld: 0.135018399247\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) recons: 7.91357631683\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) logppx: 8.04859472911\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=8.04859472911\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:08.961] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 206, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) total: 8.66396417862\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) kld: 0.117112448888\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) recons: 8.54685187707\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Loss (name: value) logppx: 8.66396417862\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] #validation_score (22): 8.663964178623297\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] patience losses:[8.63551033215645, 8.672447595840845, 8.637456610263921, 8.69089136368189, 8.64257323436248] min patience loss:8.63551033216 current loss:8.66396417862 absolute loss difference:0.0284538464668\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] Timing: train: 1.50s, val: 0.21s, epoch: 1.71s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 23364, \"sum\": 23364.0, \"min\": 23364}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1567383308.962663, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1567383307.255263}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=621.947697397 records/second\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] \u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:08 INFO 139681252058944] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:09.121] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 2941, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] # Finished training epoch 12 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) total: 8.24290393955\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) kld: 0.151517222521\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) recons: 8.09138673594\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) logppx: 8.24290393955\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=8.24290393955\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:09.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 215, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) total: 8.62860467373\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) kld: 0.147734294794\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) recons: 8.4808704474\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Loss (name: value) logppx: 8.62860467373\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] #validation_score (12): 8.628604673727965\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] patience losses:[8.656922990847857, 8.641435985076122, 8.615069814828725, 8.627538358248197, 8.613983310797275] min patience loss:8.6139833108 current loss:8.62860467373 absolute loss difference:0.0146213629307\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] Timing: train: 2.94s, val: 0.22s, epoch: 3.16s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 852, \"sum\": 852.0, \"min\": 852}, \"Total Records Seen\": {\"count\": 1, \"max\": 25512, \"sum\": 25512.0, \"min\": 25512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1567383309.339497, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1567383306.180197}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=672.904913545 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:09 INFO 140663462225728] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:10.428] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1464, \"num_examples\": 36, \"num_bytes\": 555736}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] # Finished training epoch 23 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) total: 8.03961089099\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) kld: 0.135679272148\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) recons: 7.90393167425\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) logppx: 8.03961089099\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=8.03961089099\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:15:10.629] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 199, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) total: 8.65034289238\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) kld: 0.14533900481\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) recons: 8.50500386556\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Loss (name: value) logppx: 8.65034289238\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] #validation_score (23): 8.650342892377804\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] patience losses:[8.672447595840845, 8.637456610263921, 8.69089136368189, 8.64257323436248, 8.663964178623297] min patience loss:8.63745661026 current loss:8.65034289238 absolute loss difference:0.0128862821139\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Timing: train: 1.47s, val: 0.20s, epoch: 1.67s\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 24426, \"sum\": 24426.0, \"min\": 24426}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1567383310.630739, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1567383308.962947}\n",
      "\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:15:10 INFO 139681252058944] #throughput_metric: host=algo-2, train throughput=636.716227895 records/second\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:11.935] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2595, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] # Finished training epoch 13 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] Loss (name: value) total: 8.2197476884\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] Loss (name: value) kld: 0.155274693507\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] Loss (name: value) recons: 8.0644730259\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] Loss (name: value) logppx: 8.2197476884\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:11 INFO 140663462225728] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=8.2197476884\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:12.093] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 156, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Loss (name: value) total: 8.62311604818\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Loss (name: value) kld: 0.132812650998\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Loss (name: value) recons: 8.49030331343\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Loss (name: value) logppx: 8.62311604818\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] #validation_score (13): 8.623116048177083\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] patience losses:[8.641435985076122, 8.615069814828725, 8.627538358248197, 8.613983310797275, 8.628604673727965] min patience loss:8.6139833108 current loss:8.62311604818 absolute loss difference:0.00913273737981\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] Timing: train: 2.60s, val: 0.16s, epoch: 2.75s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 923, \"sum\": 923.0, \"min\": 923}, \"Total Records Seen\": {\"count\": 1, \"max\": 27638, \"sum\": 27638.0, \"min\": 27638}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1567383312.094344, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1567383309.339728}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=771.758136202 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:12 INFO 140663462225728] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:14.643] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 2548, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] # Finished training epoch 14 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) total: 8.15979063365\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) kld: 0.159642574037\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) recons: 8.00014804607\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) logppx: 8.15979063365\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=8.15979063365\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:14.802] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 158, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) total: 8.5924891154\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) kld: 0.147751844235\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) recons: 8.44473712628\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Loss (name: value) logppx: 8.5924891154\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] #validation_score (14): 8.592489115397136\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] patience losses:[8.615069814828725, 8.627538358248197, 8.613983310797275, 8.628604673727965, 8.623116048177083] min patience loss:8.6139833108 current loss:8.5924891154 absolute loss difference:0.0214941954001\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] Timing: train: 2.55s, val: 0.16s, epoch: 2.71s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 994, \"sum\": 994.0, \"min\": 994}, \"Total Records Seen\": {\"count\": 1, \"max\": 29764, \"sum\": 29764.0, \"min\": 29764}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1567383314.807143, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1567383312.094598}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=783.727315929 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:14 INFO 140663462225728] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:17.320] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 2512, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] # Finished training epoch 15 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) total: 8.12223128914\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) kld: 0.160581598651\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) recons: 7.96164969861\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) logppx: 8.12223128914\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=8.12223128914\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:17.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 156, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) total: 8.58304490309\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) kld: 0.127753214958\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) recons: 8.45529159155\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Loss (name: value) logppx: 8.58304490309\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] #validation_score (15): 8.583044903094953\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] patience losses:[8.627538358248197, 8.613983310797275, 8.628604673727965, 8.623116048177083, 8.592489115397136] min patience loss:8.5924891154 current loss:8.58304490309 absolute loss difference:0.00944421230218\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] Timing: train: 2.51s, val: 0.16s, epoch: 2.67s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1065, \"sum\": 1065.0, \"min\": 1065}, \"Total Records Seen\": {\"count\": 1, \"max\": 31890, \"sum\": 31890.0, \"min\": 31890}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1567383317.482467, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1567383314.807386}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=794.702196812 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:17 INFO 140663462225728] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:20.035] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 2552, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] # Finished training epoch 16 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) total: 8.09834575384\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) kld: 0.162418557221\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) recons: 7.93592717704\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) logppx: 8.09834575384\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=8.09834575384\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:20.212] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 176, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) total: 8.58055846385\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) kld: 0.139238469417\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) recons: 8.44132013565\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Loss (name: value) logppx: 8.58055846385\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] #validation_score (16): 8.580558463854667\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] patience losses:[8.613983310797275, 8.628604673727965, 8.623116048177083, 8.592489115397136, 8.583044903094953] min patience loss:8.58304490309 current loss:8.58055846385 absolute loss difference:0.00248643924029\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] Timing: train: 2.55s, val: 0.18s, epoch: 2.74s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1136, \"sum\": 1136.0, \"min\": 1136}, \"Total Records Seen\": {\"count\": 1, \"max\": 34016, \"sum\": 34016.0, \"min\": 34016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1567383320.218359, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1567383317.482715}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=777.1064259 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:20 INFO 140663462225728] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:22.707] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 2488, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] # Finished training epoch 17 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) total: 8.07778930664\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) kld: 0.168310997408\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) recons: 7.90947830845\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) logppx: 8.07778930664\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=8.07778930664\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:22.886] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 177, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) total: 8.5644730788\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) kld: 0.136569862488\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) recons: 8.42790304331\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Loss (name: value) logppx: 8.5644730788\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] #validation_score (17): 8.564473078801083\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] patience losses:[8.628604673727965, 8.623116048177083, 8.592489115397136, 8.583044903094953, 8.580558463854667] min patience loss:8.58055846385 current loss:8.5644730788 absolute loss difference:0.0160853850536\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] Timing: train: 2.49s, val: 0.18s, epoch: 2.67s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1207, \"sum\": 1207.0, \"min\": 1207}, \"Total Records Seen\": {\"count\": 1, \"max\": 36142, \"sum\": 36142.0, \"min\": 36142}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1567383322.891603, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1567383320.218647}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=795.331832413 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:22 INFO 140663462225728] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:25.372] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 2479, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] # Finished training epoch 18 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) total: 8.0533418539\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) kld: 0.17029142682\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) recons: 7.88305042249\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) logppx: 8.0533418539\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=8.0533418539\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:25.530] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 156, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) total: 8.55991680439\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) kld: 0.146554094094\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) recons: 8.41336290408\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Loss (name: value) logppx: 8.55991680439\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] #validation_score (18): 8.559916804387019\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] patience losses:[8.623116048177083, 8.592489115397136, 8.583044903094953, 8.580558463854667, 8.564473078801083] min patience loss:8.5644730788 current loss:8.55991680439 absolute loss difference:0.00455627441406\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] Timing: train: 2.48s, val: 0.16s, epoch: 2.64s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1278, \"sum\": 1278.0, \"min\": 1278}, \"Total Records Seen\": {\"count\": 1, \"max\": 38268, \"sum\": 38268.0, \"min\": 38268}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1567383325.534809, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1567383322.891848}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=804.350602747 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:25 INFO 140663462225728] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:28.019] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 2484, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] # Finished training epoch 19 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) total: 8.02825932749\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) kld: 0.171932507793\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) recons: 7.85632675243\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) logppx: 8.02825932749\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=8.02825932749\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:28.218] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 198, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) total: 8.55368879269\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) kld: 0.161611752632\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) recons: 8.39207708897\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Loss (name: value) logppx: 8.55368879269\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] #validation_score (19): 8.55368879269331\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] patience losses:[8.592489115397136, 8.583044903094953, 8.580558463854667, 8.564473078801083, 8.559916804387019] min patience loss:8.55991680439 current loss:8.55368879269 absolute loss difference:0.00622801169371\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] Timing: train: 2.49s, val: 0.20s, epoch: 2.69s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1349, \"sum\": 1349.0, \"min\": 1349}, \"Total Records Seen\": {\"count\": 1, \"max\": 40394, \"sum\": 40394.0, \"min\": 40394}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1567383328.224464, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1567383325.535088}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=790.474186984 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:28 INFO 140663462225728] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:30.733] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 2508, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] # Finished training epoch 20 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) total: 8.01640597778\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) kld: 0.179782126207\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) recons: 7.83662386612\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) logppx: 8.01640597778\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=8.01640597778\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:30.906] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 172, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) total: 8.54918510241\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) kld: 0.158807314971\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) recons: 8.39037765112\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Loss (name: value) logppx: 8.54918510241\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] #validation_score (20): 8.549185102413862\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] patience losses:[8.583044903094953, 8.580558463854667, 8.564473078801083, 8.559916804387019, 8.55368879269331] min patience loss:8.55368879269 current loss:8.54918510241 absolute loss difference:0.00450369027945\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] Timing: train: 2.51s, val: 0.18s, epoch: 2.69s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1420, \"sum\": 1420.0, \"min\": 1420}, \"Total Records Seen\": {\"count\": 1, \"max\": 42520, \"sum\": 42520.0, \"min\": 42520}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1567383330.911233, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1567383328.22477}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=791.333938742 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:30 INFO 140663462225728] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:33.422] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 2510, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] # Finished training epoch 21 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) total: 7.99977202796\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) kld: 0.181712709794\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) recons: 7.8180593213\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) logppx: 7.99977202796\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=7.99977202796\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:33.580] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 156, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) total: 8.55299377441\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) kld: 0.16213728648\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) recons: 8.39085650322\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Loss (name: value) logppx: 8.55299377441\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] #validation_score (21): 8.552993774414062\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] patience losses:[8.580558463854667, 8.564473078801083, 8.559916804387019, 8.55368879269331, 8.549185102413862] min patience loss:8.54918510241 current loss:8.55299377441 absolute loss difference:0.0038086720002\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] Timing: train: 2.51s, val: 0.16s, epoch: 2.67s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1491, \"sum\": 1491.0, \"min\": 1491}, \"Total Records Seen\": {\"count\": 1, \"max\": 44646, \"sum\": 44646.0, \"min\": 44646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1567383333.58139, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1567383330.911477}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=796.241860771 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:33 INFO 140663462225728] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:36.102] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 2520, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] # Finished training epoch 22 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) total: 7.98163742997\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) kld: 0.179193567894\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) recons: 7.80244383476\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) logppx: 7.98163742997\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=7.98163742997\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:36.272] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 169, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) total: 8.52726029616\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) kld: 0.145163171108\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) recons: 8.38209709754\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Loss (name: value) logppx: 8.52726029616\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] #validation_score (22): 8.527260296161359\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] patience losses:[8.564473078801083, 8.559916804387019, 8.55368879269331, 8.549185102413862, 8.552993774414062] min patience loss:8.54918510241 current loss:8.52726029616 absolute loss difference:0.0219248062525\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] Timing: train: 2.52s, val: 0.18s, epoch: 2.70s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1562, \"sum\": 1562.0, \"min\": 1562}, \"Total Records Seen\": {\"count\": 1, \"max\": 46772, \"sum\": 46772.0, \"min\": 46772}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1567383336.278371, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1567383333.581657}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=788.324177331 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:36 INFO 140663462225728] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:38.838] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 2559, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] # Finished training epoch 23 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) total: 7.97270493485\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) kld: 0.18134348885\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) recons: 7.79136145507\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) logppx: 7.97270493485\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=7.97270493485\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:38.997] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 158, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) total: 8.52258742895\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) kld: 0.150245799162\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) recons: 8.37234168419\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] Loss (name: value) logppx: 8.52258742895\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:38 INFO 140663462225728] #validation_score (23): 8.522587428948817\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] patience losses:[8.559916804387019, 8.55368879269331, 8.549185102413862, 8.552993774414062, 8.527260296161359] min patience loss:8.52726029616 current loss:8.52258742895 absolute loss difference:0.00467286721254\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] Timing: train: 2.56s, val: 0.16s, epoch: 2.72s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1633, \"sum\": 1633.0, \"min\": 1633}, \"Total Records Seen\": {\"count\": 1, \"max\": 48898, \"sum\": 48898.0, \"min\": 48898}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1567383339.002585, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1567383336.27864}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=780.447020332 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:39 INFO 140663462225728] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:41.551] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 2548, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] # Finished training epoch 24 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) total: 7.9611648273\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) kld: 0.182600971343\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) recons: 7.77856385137\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) logppx: 7.9611648273\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=7.9611648273\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:41.712] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 159, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) total: 8.5153497158\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) kld: 0.162026560001\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) recons: 8.35332316863\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Loss (name: value) logppx: 8.5153497158\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] #validation_score (24): 8.515349715795272\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] patience losses:[8.55368879269331, 8.549185102413862, 8.552993774414062, 8.527260296161359, 8.522587428948817] min patience loss:8.52258742895 current loss:8.5153497158 absolute loss difference:0.00723771315355\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] Timing: train: 2.55s, val: 0.16s, epoch: 2.71s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1704, \"sum\": 1704.0, \"min\": 1704}, \"Total Records Seen\": {\"count\": 1, \"max\": 51024, \"sum\": 51024.0, \"min\": 51024}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1567383341.71679, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1567383339.002822}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=783.317337201 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:41 INFO 140663462225728] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:44.313] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 2595, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] # Finished training epoch 25 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) total: 7.95353081215\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) kld: 0.184027053829\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) recons: 7.76950376269\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) logppx: 7.95353081215\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=7.95353081215\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:44.485] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 171, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) total: 8.5170126108\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) kld: 0.163836212647\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) recons: 8.35317629301\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Loss (name: value) logppx: 8.5170126108\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] #validation_score (25): 8.517012610802283\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] patience losses:[8.549185102413862, 8.552993774414062, 8.527260296161359, 8.522587428948817, 8.515349715795272] min patience loss:8.5153497158 current loss:8.5170126108 absolute loss difference:0.00166289500701\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] Timing: train: 2.60s, val: 0.17s, epoch: 2.77s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1775, \"sum\": 1775.0, \"min\": 1775}, \"Total Records Seen\": {\"count\": 1, \"max\": 53150, \"sum\": 53150.0, \"min\": 53150}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1567383344.486619, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1567383341.717033}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=767.589104597 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:44 INFO 140663462225728] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:47.003] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 2516, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] # Finished training epoch 26 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) total: 7.94540110843\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) kld: 0.185766527351\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) recons: 7.75963455702\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) logppx: 7.94540110843\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=7.94540110843\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:47.185] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 179, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) total: 8.52463938395\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) kld: 0.155244144415\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) recons: 8.36939525115\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Loss (name: value) logppx: 8.52463938395\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] #validation_score (26): 8.524639383951824\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] patience losses:[8.552993774414062, 8.527260296161359, 8.522587428948817, 8.515349715795272, 8.517012610802283] min patience loss:8.5153497158 current loss:8.52463938395 absolute loss difference:0.00928966815655\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] Timing: train: 2.52s, val: 0.18s, epoch: 2.70s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1846, \"sum\": 1846.0, \"min\": 1846}, \"Total Records Seen\": {\"count\": 1, \"max\": 55276, \"sum\": 55276.0, \"min\": 55276}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1567383347.186441, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1567383344.486847}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=787.483815018 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:47 INFO 140663462225728] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:49.806] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 2619, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] # Finished training epoch 27 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) total: 7.93302702568\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) kld: 0.182016583228\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) recons: 7.75101049719\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) logppx: 7.93302702568\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=7.93302702568\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:49.990] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 183, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) total: 8.51004834297\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) kld: 0.149755262106\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) recons: 8.36029303135\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Loss (name: value) logppx: 8.51004834297\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] #validation_score (27): 8.510048342973757\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] patience losses:[8.527260296161359, 8.522587428948817, 8.515349715795272, 8.517012610802283, 8.524639383951824] min patience loss:8.5153497158 current loss:8.51004834297 absolute loss difference:0.00530137282152\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] Timing: train: 2.62s, val: 0.19s, epoch: 2.81s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1917, \"sum\": 1917.0, \"min\": 1917}, \"Total Records Seen\": {\"count\": 1, \"max\": 57402, \"sum\": 57402.0, \"min\": 57402}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1567383349.996747, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1567383347.186714}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=756.537107909 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:49 INFO 140663462225728] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:52.537] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 2539, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] # Finished training epoch 28 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) total: 7.93124108829\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) kld: 0.187662312794\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) recons: 7.74357878636\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) logppx: 7.93124108829\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=7.93124108829\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:52.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 171, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) total: 8.51794288831\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) kld: 0.158172446031\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) recons: 8.35977039826\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Loss (name: value) logppx: 8.51794288831\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] #validation_score (28): 8.517942888308793\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] patience losses:[8.522587428948817, 8.515349715795272, 8.517012610802283, 8.524639383951824, 8.510048342973757] min patience loss:8.51004834297 current loss:8.51794288831 absolute loss difference:0.00789454533504\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] Timing: train: 2.54s, val: 0.17s, epoch: 2.71s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1988, \"sum\": 1988.0, \"min\": 1988}, \"Total Records Seen\": {\"count\": 1, \"max\": 59528, \"sum\": 59528.0, \"min\": 59528}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1567383352.710368, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1567383349.99703}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=783.496284383 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:52 INFO 140663462225728] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:55.280] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 2569, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] # Finished training epoch 29 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) total: 7.92353347277\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) kld: 0.187870443819\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) recons: 7.73566302805\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) logppx: 7.92353347277\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=7.92353347277\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:55.470] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 189, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) total: 8.52306929368\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) kld: 0.171710661741\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) recons: 8.35135850173\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Loss (name: value) logppx: 8.52306929368\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] #validation_score (29): 8.523069293682392\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] patience losses:[8.515349715795272, 8.517012610802283, 8.524639383951824, 8.510048342973757, 8.517942888308793] min patience loss:8.51004834297 current loss:8.52306929368 absolute loss difference:0.0130209507086\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] Timing: train: 2.57s, val: 0.19s, epoch: 2.76s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2059, \"sum\": 2059.0, \"min\": 2059}, \"Total Records Seen\": {\"count\": 1, \"max\": 61654, \"sum\": 61654.0, \"min\": 61654}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1567383355.471566, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1567383352.710654}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=769.997723275 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:55 INFO 140663462225728] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:58.035] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 2563, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] # Finished training epoch 30 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) total: 7.91707238569\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) kld: 0.190937300803\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) recons: 7.72613508914\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) logppx: 7.91707238569\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=7.91707238569\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:15:58.219] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 182, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) total: 8.51285869892\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) kld: 0.158287625435\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) recons: 8.35457110283\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Loss (name: value) logppx: 8.51285869892\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] #validation_score (30): 8.512858698918269\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] patience losses:[8.517012610802283, 8.524639383951824, 8.510048342973757, 8.517942888308793, 8.523069293682392] min patience loss:8.51004834297 current loss:8.51285869892 absolute loss difference:0.00281035594451\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] Timing: train: 2.56s, val: 0.18s, epoch: 2.75s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2130, \"sum\": 2130.0, \"min\": 2130}, \"Total Records Seen\": {\"count\": 1, \"max\": 63780, \"sum\": 63780.0, \"min\": 63780}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1567383358.221047, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1567383355.471849}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=773.27761929 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:15:58 INFO 140663462225728] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:00.827] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 2605, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] # Finished training epoch 31 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) total: 7.9076557643\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) kld: 0.186479234024\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) recons: 7.72117650565\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) logppx: 7.9076557643\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=7.9076557643\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:00.988] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 160, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) total: 8.50178735195\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) kld: 0.154825937442\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) recons: 8.34696146647\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Loss (name: value) logppx: 8.50178735195\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] #validation_score (31): 8.501787351950622\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] patience losses:[8.524639383951824, 8.510048342973757, 8.517942888308793, 8.523069293682392, 8.512858698918269] min patience loss:8.51004834297 current loss:8.50178735195 absolute loss difference:0.00826099102314\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] Timing: train: 2.61s, val: 0.17s, epoch: 2.77s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2201, \"sum\": 2201.0, \"min\": 2201}, \"Total Records Seen\": {\"count\": 1, \"max\": 65906, \"sum\": 65906.0, \"min\": 65906}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1567383360.994054, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1567383358.221331}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=766.715207092 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:00 INFO 140663462225728] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:03.526] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 2532, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] # Finished training epoch 32 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) total: 7.90519548156\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) kld: 0.190290885128\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) recons: 7.71490457741\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) logppx: 7.90519548156\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=7.90519548156\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:03.685] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 95, \"duration\": 156, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) total: 8.50730841221\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) kld: 0.175245583363\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) recons: 8.33206274571\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Loss (name: value) logppx: 8.50730841221\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] #validation_score (32): 8.507308412209536\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] patience losses:[8.510048342973757, 8.517942888308793, 8.523069293682392, 8.512858698918269, 8.501787351950622] min patience loss:8.50178735195 current loss:8.50730841221 absolute loss difference:0.00552106025891\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] Timing: train: 2.53s, val: 0.16s, epoch: 2.69s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2272, \"sum\": 2272.0, \"min\": 2272}, \"Total Records Seen\": {\"count\": 1, \"max\": 68032, \"sum\": 68032.0, \"min\": 68032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1567383363.686446, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1567383360.994341}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=789.675249365 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:03 INFO 140663462225728] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:06.286] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 2599, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] # Finished training epoch 33 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) total: 7.90042671598\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) kld: 0.191382591154\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) recons: 7.70904412068\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) logppx: 7.90042671598\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=7.90042671598\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:06.451] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 98, \"duration\": 163, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) total: 8.50349101531\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) kld: 0.165747046471\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) recons: 8.337743945\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Loss (name: value) logppx: 8.50349101531\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] #validation_score (33): 8.503491015312\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] patience losses:[8.517942888308793, 8.523069293682392, 8.512858698918269, 8.501787351950622, 8.507308412209536] min patience loss:8.50178735195 current loss:8.50349101531 absolute loss difference:0.00170366336138\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] Timing: train: 2.60s, val: 0.16s, epoch: 2.77s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2343, \"sum\": 2343.0, \"min\": 2343}, \"Total Records Seen\": {\"count\": 1, \"max\": 70158, \"sum\": 70158.0, \"min\": 70158}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1567383366.453032, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1567383363.686697}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=768.484732198 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:06 INFO 140663462225728] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:09.002] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 2548, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] # Finished training epoch 34 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) total: 7.8964498887\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) kld: 0.192349855441\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) recons: 7.70410002946\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) logppx: 7.8964498887\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=7.8964498887\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:09.161] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 101, \"duration\": 158, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) total: 8.4956947131\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) kld: 0.169322211314\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) recons: 8.32637258676\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Loss (name: value) logppx: 8.4956947131\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] #validation_score (34): 8.495694713103466\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] patience losses:[8.523069293682392, 8.512858698918269, 8.501787351950622, 8.507308412209536, 8.503491015312] min patience loss:8.50178735195 current loss:8.4956947131 absolute loss difference:0.00609263884716\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] Timing: train: 2.55s, val: 0.16s, epoch: 2.71s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2414, \"sum\": 2414.0, \"min\": 2414}, \"Total Records Seen\": {\"count\": 1, \"max\": 72284, \"sum\": 72284.0, \"min\": 72284}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1567383369.166776, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1567383366.453327}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=783.463654842 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:09 INFO 140663462225728] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:11.730] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 2562, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] # Finished training epoch 35 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) total: 7.88989046482\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) kld: 0.193432434736\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) recons: 7.69645801956\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) logppx: 7.88989046482\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=7.88989046482\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:11.889] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 104, \"duration\": 158, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) total: 8.49349509997\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) kld: 0.172062583459\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) recons: 8.32143245599\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Loss (name: value) logppx: 8.49349509997\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] #validation_score (35): 8.493495099972456\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] patience losses:[8.512858698918269, 8.501787351950622, 8.507308412209536, 8.503491015312, 8.495694713103466] min patience loss:8.4956947131 current loss:8.49349509997 absolute loss difference:0.00219961313101\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] Timing: train: 2.56s, val: 0.16s, epoch: 2.73s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2485, \"sum\": 2485.0, \"min\": 2485}, \"Total Records Seen\": {\"count\": 1, \"max\": 74410, \"sum\": 74410.0, \"min\": 74410}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1567383371.894452, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1567383369.167091}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=779.467298369 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:11 INFO 140663462225728] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:14.485] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 2590, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] # Finished training epoch 36 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) total: 7.88147719835\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) kld: 0.190004107202\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) recons: 7.69147313794\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) logppx: 7.88147719835\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=7.88147719835\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:14.653] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 107, \"duration\": 166, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) total: 8.49255120693\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) kld: 0.17537811964\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) recons: 8.31717306284\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Loss (name: value) logppx: 8.49255120693\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] #validation_score (36): 8.49255120693109\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] patience losses:[8.501787351950622, 8.507308412209536, 8.503491015312, 8.495694713103466, 8.493495099972456] min patience loss:8.49349509997 current loss:8.49255120693 absolute loss difference:0.000943893041367\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] Timing: train: 2.59s, val: 0.17s, epoch: 2.76s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 76536, \"sum\": 76536.0, \"min\": 76536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1567383374.658895, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1567383371.894708}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=769.082713978 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:14 INFO 140663462225728] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:17.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 2584, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] # Finished training epoch 37 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) total: 7.87818671571\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) kld: 0.193707754578\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) recons: 7.68447898901\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) logppx: 7.87818671571\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=7.87818671571\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:17.404] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 110, \"duration\": 159, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) total: 8.49208526611\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) kld: 0.183098169474\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) recons: 8.30898711376\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Loss (name: value) logppx: 8.49208526611\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] #validation_score (37): 8.492085266113282\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] patience losses:[8.507308412209536, 8.503491015312, 8.495694713103466, 8.493495099972456, 8.49255120693109] min patience loss:8.49255120693 current loss:8.49208526611 absolute loss difference:0.000465940817808\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] Timing: train: 2.59s, val: 0.16s, epoch: 2.75s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2627, \"sum\": 2627.0, \"min\": 2627}, \"Total Records Seen\": {\"count\": 1, \"max\": 78662, \"sum\": 78662.0, \"min\": 78662}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1567383377.409639, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1567383374.659192}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=772.921439577 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:17 INFO 140663462225728] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:19.979] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 2569, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] # Finished training epoch 38 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] Loss (name: value) total: 7.87624045359\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] Loss (name: value) kld: 0.196493552884\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] Loss (name: value) recons: 7.67974688749\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] Loss (name: value) logppx: 7.87624045359\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:19 INFO 140663462225728] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=7.87624045359\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:20.177] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 113, \"duration\": 196, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Loss (name: value) total: 8.48438380315\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Loss (name: value) kld: 0.177874032045\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Loss (name: value) recons: 8.30650963416\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Loss (name: value) logppx: 8.48438380315\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] #validation_score (38): 8.484383803147535\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] patience losses:[8.503491015312, 8.495694713103466, 8.493495099972456, 8.49255120693109, 8.492085266113282] min patience loss:8.49208526611 current loss:8.48438380315 absolute loss difference:0.00770146296575\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] Timing: train: 2.57s, val: 0.20s, epoch: 2.77s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2698, \"sum\": 2698.0, \"min\": 2698}, \"Total Records Seen\": {\"count\": 1, \"max\": 80788, \"sum\": 80788.0, \"min\": 80788}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1567383380.183462, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1567383377.409927}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=766.4904714 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:20 INFO 140663462225728] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:22.751] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 2567, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] # Finished training epoch 39 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) total: 7.86899956358\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) kld: 0.195151881769\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) recons: 7.67384768204\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) logppx: 7.86899956358\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=7.86899956358\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:22.907] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 116, \"duration\": 154, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) total: 8.49262754\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) kld: 0.180112275099\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) recons: 8.31251529791\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Loss (name: value) logppx: 8.49262754\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] #validation_score (39): 8.492627540001502\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] patience losses:[8.495694713103466, 8.493495099972456, 8.49255120693109, 8.492085266113282, 8.484383803147535] min patience loss:8.48438380315 current loss:8.49262754 absolute loss difference:0.00824373685397\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] Timing: train: 2.57s, val: 0.16s, epoch: 2.72s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2769, \"sum\": 2769.0, \"min\": 2769}, \"Total Records Seen\": {\"count\": 1, \"max\": 82914, \"sum\": 82914.0, \"min\": 82914}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1567383382.908532, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1567383380.183749}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=780.20610847 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:22 INFO 140663462225728] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:25.497] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 2588, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] # Finished training epoch 40 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) total: 7.86329462472\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) kld: 0.195154825175\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) recons: 7.66813977739\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) logppx: 7.86329462472\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=7.86329462472\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:25.660] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 119, \"duration\": 162, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) total: 8.48158080272\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) kld: 0.173475451347\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) recons: 8.30810535137\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Loss (name: value) logppx: 8.48158080272\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] #validation_score (40): 8.481580802721854\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] patience losses:[8.493495099972456, 8.49255120693109, 8.492085266113282, 8.484383803147535, 8.492627540001502] min patience loss:8.48438380315 current loss:8.48158080272 absolute loss difference:0.00280300042568\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] Timing: train: 2.59s, val: 0.17s, epoch: 2.76s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2840, \"sum\": 2840.0, \"min\": 2840}, \"Total Records Seen\": {\"count\": 1, \"max\": 85040, \"sum\": 85040.0, \"min\": 85040}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1567383385.665253, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1567383382.908779}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=771.237959227 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:25 INFO 140663462225728] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:28.189] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 2523, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] # Finished training epoch 41 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) total: 7.85968968924\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) kld: 0.19783647329\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) recons: 7.66185319211\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) logppx: 7.85968968924\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=7.85968968924\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:28.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 122, \"duration\": 174, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) total: 8.48294180846\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) kld: 0.187284514843\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) recons: 8.29565738776\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Loss (name: value) logppx: 8.48294180846\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] #validation_score (41): 8.48294180845603\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] patience losses:[8.49255120693109, 8.492085266113282, 8.484383803147535, 8.492627540001502, 8.481580802721854] min patience loss:8.48158080272 current loss:8.48294180846 absolute loss difference:0.00136100573418\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] Timing: train: 2.53s, val: 0.18s, epoch: 2.70s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2911, \"sum\": 2911.0, \"min\": 2911}, \"Total Records Seen\": {\"count\": 1, \"max\": 87166, \"sum\": 87166.0, \"min\": 87166}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1567383388.367048, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1567383385.665493}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=786.915424613 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:28 INFO 140663462225728] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:30.925] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 2558, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] # Finished training epoch 42 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] Loss (name: value) total: 7.85045700431\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] Loss (name: value) kld: 0.196136419426\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] Loss (name: value) recons: 7.65432058701\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] Loss (name: value) logppx: 7.85045700431\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:30 INFO 140663462225728] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=7.85045700431\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:31.112] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 125, \"duration\": 185, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Loss (name: value) total: 8.46607227815\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Loss (name: value) kld: 0.175645830692\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Loss (name: value) recons: 8.29042644012\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Loss (name: value) logppx: 8.46607227815\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] #validation_score (42): 8.466072278145033\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] patience losses:[8.492085266113282, 8.484383803147535, 8.492627540001502, 8.481580802721854, 8.48294180845603] min patience loss:8.48158080272 current loss:8.46607227815 absolute loss difference:0.0155085245768\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] Timing: train: 2.56s, val: 0.19s, epoch: 2.75s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2982, \"sum\": 2982.0, \"min\": 2982}, \"Total Records Seen\": {\"count\": 1, \"max\": 89292, \"sum\": 89292.0, \"min\": 89292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1567383391.11765, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1567383388.367295}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=772.953196888 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:31 INFO 140663462225728] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:33.662] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 2544, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] # Finished training epoch 43 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) total: 7.84606539587\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) kld: 0.19723748769\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) recons: 7.6488278886\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) logppx: 7.84606539587\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=7.84606539587\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:33.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 128, \"duration\": 160, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) total: 8.50281438583\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) kld: 0.202725733244\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) recons: 8.30008865748\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Loss (name: value) logppx: 8.50281438583\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] #validation_score (43): 8.502814385829828\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] patience losses:[8.484383803147535, 8.492627540001502, 8.481580802721854, 8.48294180845603, 8.466072278145033] min patience loss:8.46607227815 current loss:8.50281438583 absolute loss difference:0.0367421076848\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] Timing: train: 2.55s, val: 0.16s, epoch: 2.71s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3053, \"sum\": 3053.0, \"min\": 3053}, \"Total Records Seen\": {\"count\": 1, \"max\": 91418, \"sum\": 91418.0, \"min\": 91418}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1567383393.825173, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1567383391.117932}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=785.259627875 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:33 INFO 140663462225728] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:36.484] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 2659, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] # Finished training epoch 44 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) total: 7.84018294643\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) kld: 0.199906013157\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) recons: 7.64027693484\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) logppx: 7.84018294643\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=7.84018294643\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:36.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 131, \"duration\": 160, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) total: 8.48772172194\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) kld: 0.19627041939\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) recons: 8.29145128299\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Loss (name: value) logppx: 8.48772172194\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] #validation_score (44): 8.487721721942608\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] patience losses:[8.492627540001502, 8.481580802721854, 8.48294180845603, 8.466072278145033, 8.502814385829828] min patience loss:8.46607227815 current loss:8.48772172194 absolute loss difference:0.0216494437976\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] Timing: train: 2.66s, val: 0.16s, epoch: 2.82s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3124, \"sum\": 3124.0, \"min\": 3124}, \"Total Records Seen\": {\"count\": 1, \"max\": 93544, \"sum\": 93544.0, \"min\": 93544}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1567383396.647857, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1567383393.825449}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=753.22041792 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:36 INFO 140663462225728] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:39.244] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 2595, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] # Finished training epoch 45 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) total: 7.83148285772\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) kld: 0.196962719792\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) recons: 7.6345201161\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) logppx: 7.83148285772\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=7.83148285772\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:39.446] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 134, \"duration\": 201, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) total: 8.47897170629\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) kld: 0.199038487214\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) recons: 8.27993312738\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Loss (name: value) logppx: 8.47897170629\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] #validation_score (45): 8.478971706292569\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] patience losses:[8.481580802721854, 8.48294180845603, 8.466072278145033, 8.502814385829828, 8.487721721942608] min patience loss:8.46607227815 current loss:8.47897170629 absolute loss difference:0.0128994281475\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] Timing: train: 2.60s, val: 0.20s, epoch: 2.80s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3195, \"sum\": 3195.0, \"min\": 3195}, \"Total Records Seen\": {\"count\": 1, \"max\": 95670, \"sum\": 95670.0, \"min\": 95670}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1567383399.447987, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1567383396.648128}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=759.290317593 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:39 INFO 140663462225728] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:42.009] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 2561, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] # Finished training epoch 46 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) total: 7.82982614007\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) kld: 0.204181876541\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) recons: 7.62564425759\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) logppx: 7.82982614007\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=7.82982614007\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:42.176] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 137, \"duration\": 164, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) total: 8.47157541911\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) kld: 0.197139269266\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) recons: 8.27443612906\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Loss (name: value) logppx: 8.47157541911\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] #validation_score (46): 8.471575419108072\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] patience losses:[8.48294180845603, 8.466072278145033, 8.502814385829828, 8.487721721942608, 8.478971706292569] min patience loss:8.46607227815 current loss:8.47157541911 absolute loss difference:0.00550314096304\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] Timing: train: 2.56s, val: 0.17s, epoch: 2.73s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3266, \"sum\": 3266.0, \"min\": 3266}, \"Total Records Seen\": {\"count\": 1, \"max\": 97796, \"sum\": 97796.0, \"min\": 97796}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1567383402.177892, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1567383399.44824}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=778.807353292 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:42 INFO 140663462225728] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:44.758] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 2579, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] # Finished training epoch 47 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) total: 7.81754414733\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) kld: 0.19899291343\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) recons: 7.61855126323\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) logppx: 7.81754414733\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=7.81754414733\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:44.937] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 140, \"duration\": 178, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) total: 8.4736123892\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) kld: 0.199510758962\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) recons: 8.27410160945\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Loss (name: value) logppx: 8.4736123892\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] #validation_score (47): 8.473612389197717\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] patience losses:[8.466072278145033, 8.502814385829828, 8.487721721942608, 8.478971706292569, 8.471575419108072] min patience loss:8.46607227815 current loss:8.4736123892 absolute loss difference:0.00754011105268\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] Timing: train: 2.58s, val: 0.18s, epoch: 2.76s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3337, \"sum\": 3337.0, \"min\": 3337}, \"Total Records Seen\": {\"count\": 1, \"max\": 99922, \"sum\": 99922.0, \"min\": 99922}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1567383404.938754, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1567383402.178195}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=770.09733804 records/second\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] \u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:44 INFO 140663462225728] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:47.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 2626, \"num_examples\": 71, \"num_bytes\": 833232}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] # Finished training epoch 48 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) total: 7.81190652623\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) kld: 0.19949489054\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) recons: 7.61241165663\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) logppx: 7.81190652623\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=7.81190652623\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:47.744] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 143, \"duration\": 176, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) total: 8.47477193979\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) kld: 0.196894679925\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) recons: 8.27787729899\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) logppx: 8.47477193979\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] #validation_score (48): 8.474771939791166\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] patience losses:[8.502814385829828, 8.487721721942608, 8.478971706292569, 8.471575419108072, 8.473612389197717] min patience loss:8.47157541911 current loss:8.47477193979 absolute loss difference:0.00319652068309\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Timing: train: 2.63s, val: 0.18s, epoch: 2.81s\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3408, \"sum\": 3408.0, \"min\": 3408}, \"Total Records Seen\": {\"count\": 1, \"max\": 102048, \"sum\": 102048.0, \"min\": 102048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1567383407.745109, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1567383404.939049}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] #throughput_metric: host=algo-1, train throughput=757.610012619 records/second\u001b[0m\n",
      "\u001b[31m[2019-09-02 00:16:47.930] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 146, \"duration\": 177, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) total: 8.47499898275\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) kld: 0.189580901464\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) recons: 8.28541795779\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss (name: value) logppx: 8.47499898275\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] #quality_metric: host=algo-1, epoch=48, validation total_loss <loss>=8.47499898275\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Loss of server-side model: 8.47499898275\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Best model based on early stopping at epoch 42. Best loss: 8.46607227815\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Topics from epoch:final (num_topics:3) [wetc 0.26, tu 0.73]:\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] [0.26, 0.60] resource pending request create acceptance admin local type permanent nahoutrdhoustonpwrcommonelectric nahoutrdhoustonpwrcommonpower2region approval kobra click application date directory read risk tail\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] [0.27, 1.00] andor instruction notification reserves buy responsible sources prohibited order securities downgraded based intelligence strong corp web solicitation privacy clicking coverage\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] [0.26, 0.60] request resource pending create approval type application date tail directory acceptance admin flip permanent counterparty head click swap kobra risk\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Saved checkpoint to \"/tmp/tmpDOx6sD/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[09/02/2019 00:16:47 INFO 140663462225728] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 151282.98997879028, \"sum\": 151282.98997879028, \"min\": 151282.98997879028}, \"finalize.time\": {\"count\": 1, \"max\": 213.84596824645996, \"sum\": 213.84596824645996, \"min\": 213.84596824645996}, \"initialize.time\": {\"count\": 1, \"max\": 15582.26203918457, \"sum\": 15582.26203918457, \"min\": 15582.26203918457}, \"model.serialize.time\": {\"count\": 1, \"max\": 4.173040390014648, \"sum\": 4.173040390014648, \"min\": 4.173040390014648}, \"setuptime\": {\"count\": 1, \"max\": 47.74284362792969, \"sum\": 47.74284362792969, \"min\": 47.74284362792969}, \"early_stop.time\": {\"count\": 48, \"max\": 270.2639102935791, \"sum\": 8605.361938476562, \"min\": 155.6239128112793}, \"update.time\": {\"count\": 48, \"max\": 3222.91898727417, \"sum\": 135339.51711654663, \"min\": 2642.8070068359375}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 49, \"max\": 264.88304138183594, \"sum\": 8667.18864440918, \"min\": 155.2870273590088}}, \"EndTime\": 1567383407.964861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567383256.780556}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-09-02 00:16:47.923] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 169, \"num_examples\": 14, \"num_bytes\": 125284}\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Loss (name: value) total: 8.46190510285\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Loss (name: value) kld: 0.188610469378\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Loss (name: value) recons: 8.27329469338\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Loss (name: value) logppx: 8.46190510285\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] #quality_metric: host=algo-2, epoch=23, validation total_loss <loss>=8.46190510285\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Loss of server-side model: 8.46190510285\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Best model based on early stopping at epoch 23. Best loss: 8.46190510285\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Topics from epoch:final (num_topics:3) [wetc 0.27, tu 0.80]:\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] [0.24, 0.70] resource pending create acceptance type request admin local permanent nahoutrdhoustonpwrcommonelectric nahoutrdhoustonpwrcommonpower2region kobra application approval click date chargenbspnbspintercontinentalexchange intercontinentalexchanges directory sellside\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] [0.30, 1.00] andor instruction notification downgraded intelligence reserves buy securities responsible sources strong solicitation prohibited coverage based order privacy upgraded contained web\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] [0.26, 0.70] request resource pending approval create type application tail directory acceptance date admin counterparty flip permanent head america_corp north_america risk game\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Saved checkpoint to \"/tmp/tmpWvSnko/state-0001.params\"\u001b[0m\n",
      "\u001b[32m[09/02/2019 00:16:47 INFO 139681252058944] Test data is not provided.\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 156109.6420288086, \"sum\": 156109.6420288086, \"min\": 156109.6420288086}, \"finalize.time\": {\"count\": 1, \"max\": 206.63189888000488, \"sum\": 206.63189888000488, \"min\": 206.63189888000488}, \"initialize.time\": {\"count\": 1, \"max\": 18910.723209381104, \"sum\": 18910.723209381104, \"min\": 18910.723209381104}, \"model.serialize.time\": {\"count\": 1, \"max\": 7.160186767578125, \"sum\": 7.160186767578125, \"min\": 7.160186767578125}, \"setuptime\": {\"count\": 1, \"max\": 1560.7109069824219, \"sum\": 1560.7109069824219, \"min\": 1560.7109069824219}, \"early_stop.time\": {\"count\": 23, \"max\": 277.6148319244385, \"sum\": 4463.189601898193, \"min\": 173.39301109313965}, \"update.time\": {\"count\": 23, \"max\": 1896.2619304656982, \"sum\": 38231.66227340698, \"min\": 1569.2570209503174}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 24, \"max\": 273.82898330688477, \"sum\": 4597.809076309204, \"min\": 170.2730655670166}}, \"EndTime\": 1567383407.96877, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1567383253.470877}\n",
      "\u001b[0m\n",
      "\n",
      "2019-09-02 00:16:57 Uploading - Uploading generated training model\n",
      "2019-09-02 00:16:57 Completed - Training job completed\n",
      "Billable seconds: 433\n"
     ]
    }
   ],
   "source": [
    "ntm_estmtr.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2019-04-09-17-24-19-713\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm_estmtr.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hosting and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can either deploy the trained model using sagemaker estimator object or create a sagemaker and then invoke deploy() method\n",
    "\n",
    "ntm_predctr = ntm_estmtr.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "# OR \n",
    "\n",
    "# current_job_name = 'ntm-2019-09-02-00-11-12-089'\n",
    "# model_path = os.path.join('s3://', bucket, output_prefix, current_job_name, 'output/model.tar.gz')\n",
    "# ntm_model = Model(model_data=model_path, image=container, role=role, sagemaker_session=sess)\n",
    "# ntm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "# ntm_predctr = sagemaker.predictor.RealTimePredictor(\n",
    "#     ntm_model.endpoint_name, \n",
    "#     sagemaker_session=sess\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: ntm-2019-09-02-11-59-50-170\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint name: {}'.format(ntm_model.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predctr.content_type = 'text/csv'\n",
    "ntm_predctr.serializer = csv_serializer\n",
    "ntm_predctr.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test vectors from compressed sparse matrix to dense matrix\n",
    "test_data = np.array(test_data.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ntm_predctr.predict(test_data[1:6])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04416979 0.50438106 0.45144916]\n",
      " [0.03966784 0.47783577 0.48249644]\n",
      " [0.0343897  0.47826818 0.48734212]\n",
      " [0.04246533 0.72791708 0.22961761]\n",
      " [0.04536712 0.29173526 0.66289765]]\n"
     ]
    }
   ],
   "source": [
    "topic_wts_res = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(topic_wts_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic Number')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fnt_sz=14\n",
    "\n",
    "df_tpcwts=pd.DataFrame(topic_wts_res.T)\n",
    "\n",
    "df_tpcwts.plot(kind='bar', figsize=(16,4), fontsize=fnt_sz)\n",
    "\n",
    "plt.ylabel('Topic % Across Emails', fontsize=fnt_sz)\n",
    "plt.xlabel('Topic Number', fontsize=fnt_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Cloud from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/08/186a7d67998f1e38d6d853c71c149820983c547804348f06727f552df20d/mxnet-1.5.0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.4MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (2.20.0)\n",
      "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/92/57179ed45307ec6179e344231c47da7f3f3da9e2eee5c8ab506bd279ce4e/numpy-1.17.1-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.4MB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\n",
      "Installing collected packages: graphviz, numpy, mxnet\n",
      "  Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.5.0 numpy-1.17.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting wordcloud\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/af/849edf14d573eba9c8082db898ff0d090428d9485371cc4fe21a66717ad2/wordcloud-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (361kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368kB 24.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (1.17.1)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (5.2.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.5.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install wordcloud\n",
    "import wordcloud as wc\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enronemails/output/ntm-2019-04-09-17-24-19-713/output/model.tar.gz'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine the location of the model output\n",
    "current_job_name = ntm_estmtr.latest_training_job.job_name\n",
    "\n",
    "model_path = os.path.join(output_prefix, current_job_name, 'output/model.tar.gz')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the model\n",
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_algo-2\r\n",
      "model_algo-1\r\n"
     ]
    }
   ],
   "source": [
    "#unzip the model output\n",
    "!tar -xzvf 'downloaded_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  model_algo-1\r\n",
      " extracting: meta.json               \r\n",
      " extracting: symbol.json             \r\n",
      " extracting: params                  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o model_algo-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = mx.ndarray.load('params')\n",
    "W = model_dict['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.6004856 -2.60862   -1.9256244]\n",
       " [-1.5195873 -2.801778  -2.5669572]\n",
       " [-1.5795462 -2.8432672 -2.3337717]\n",
       " ...\n",
       " [-1.5052562 -2.8511627 -2.35806  ]\n",
       " [-1.5185319 -2.8104594 -2.4379675]\n",
       " [-1.5643051 -2.8513684 -2.3724968]]\n",
       "<NDArray 17524x3 @cpu(0)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving word distributions for each of the latent topics \n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17524"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary list\n",
    "vocab_list = pd.read_table(vocab_op_fn, header=None)\n",
    "vocab_list = vocab_list[0].tolist()\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the vocabulary list to create dictionary of key values for each of the words in vocabulary\n",
    "word_to_id = {}\n",
    "\n",
    "for i, v in enumerate(vocab_list):\n",
    "    #print(\"Index and Value\", i, v)\n",
    "    word_to_id[v] = i\n",
    "    \n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aaa': 0,\n",
       " 'aactive': 1,\n",
       " 'aadvantage': 2,\n",
       " 'aaron': 3,\n",
       " 'aarp': 4,\n",
       " 'aba': 5,\n",
       " 'abacus': 6,\n",
       " 'abandon': 7,\n",
       " 'abandoned': 8,\n",
       " 'abandoning': 9,\n",
       " 'abated': 10,\n",
       " 'abb': 11,\n",
       " 'abbott': 12,\n",
       " 'abbreviated': 13,\n",
       " 'abbreviation': 14,\n",
       " 'abc': 15,\n",
       " 'abel': 16,\n",
       " 'aberration': 17,\n",
       " 'abide': 18,\n",
       " 'abilene': 19,\n",
       " 'abilities': 20,\n",
       " 'ability': 21,\n",
       " 'able': 22,\n",
       " 'abn': 23,\n",
       " 'aboard': 24,\n",
       " 'abolish': 25,\n",
       " 'abolishing': 26,\n",
       " 'abound': 27,\n",
       " 'abovemarket': 28,\n",
       " 'abraham': 29,\n",
       " 'abreast': 30,\n",
       " 'abroad': 31,\n",
       " 'abrogate': 32,\n",
       " 'abrupt': 33,\n",
       " 'abruptly': 34,\n",
       " 'absence': 35,\n",
       " 'absences': 36,\n",
       " 'absent': 37,\n",
       " 'absentee': 38,\n",
       " 'absolute': 39,\n",
       " 'absolutely': 40,\n",
       " 'absorb': 41,\n",
       " 'absorbed': 42,\n",
       " 'absorbing': 43,\n",
       " 'abstract': 44,\n",
       " 'abt': 45,\n",
       " 'abundance': 46,\n",
       " 'abundant': 47,\n",
       " 'abuse': 48,\n",
       " 'abuses': 49,\n",
       " 'academia': 50,\n",
       " 'academic': 51,\n",
       " 'academy': 52,\n",
       " 'acc': 53,\n",
       " 'accelerate': 54,\n",
       " 'accelerated': 55,\n",
       " 'accelerates': 56,\n",
       " 'accelerating': 57,\n",
       " 'acceleration': 58,\n",
       " 'accent': 59,\n",
       " 'accenture': 60,\n",
       " 'accept': 61,\n",
       " 'acceptable': 62,\n",
       " 'acceptance': 63,\n",
       " 'acceptances': 64,\n",
       " 'accepted': 65,\n",
       " 'accepting': 66,\n",
       " 'access': 67,\n",
       " 'accessed': 68,\n",
       " 'accessibility': 69,\n",
       " 'accessible': 70,\n",
       " 'accessing': 71,\n",
       " 'accessories': 72,\n",
       " 'accessory': 73,\n",
       " 'accident': 74,\n",
       " 'accidental': 75,\n",
       " 'accidentally': 76,\n",
       " 'acclaimed': 77,\n",
       " 'accolades': 78,\n",
       " 'accommodate': 79,\n",
       " 'accommodating': 80,\n",
       " 'accommodation': 81,\n",
       " 'accomodate': 82,\n",
       " 'accompanied': 83,\n",
       " 'accompany': 84,\n",
       " 'accompanying': 85,\n",
       " 'accomplish': 86,\n",
       " 'accomplished': 87,\n",
       " 'accomplishing': 88,\n",
       " 'accomplishment': 89,\n",
       " 'accord': 90,\n",
       " 'accordance': 91,\n",
       " 'according': 92,\n",
       " 'accordion': 93,\n",
       " 'account': 94,\n",
       " 'accountability': 95,\n",
       " 'accountable': 96,\n",
       " 'accountant': 97,\n",
       " 'accounted': 98,\n",
       " 'accounting': 99,\n",
       " 'accredited': 100,\n",
       " 'accrual': 101,\n",
       " 'accrue': 102,\n",
       " 'accrued': 103,\n",
       " 'acct': 104,\n",
       " 'accum': 105,\n",
       " 'accumulate': 106,\n",
       " 'accuracy': 107,\n",
       " 'accurate': 108,\n",
       " 'accurately': 109,\n",
       " 'accusation': 110,\n",
       " 'accused': 111,\n",
       " 'accuses': 112,\n",
       " 'acevedo': 113,\n",
       " 'achievable': 114,\n",
       " 'achieve': 115,\n",
       " 'achieved': 116,\n",
       " 'achievement': 117,\n",
       " 'achieves': 118,\n",
       " 'achieving': 119,\n",
       " 'achilles': 120,\n",
       " 'acid': 121,\n",
       " 'acidity': 122,\n",
       " 'ackerman': 123,\n",
       " 'acknowledge': 124,\n",
       " 'acknowledged': 125,\n",
       " 'acknowledges': 126,\n",
       " 'acknowledging': 127,\n",
       " 'acknowledgment': 128,\n",
       " 'acn': 129,\n",
       " 'acquaintance': 130,\n",
       " 'acquire': 131,\n",
       " 'acquired': 132,\n",
       " 'acquirer': 133,\n",
       " 'acquires': 134,\n",
       " 'acquiring': 135,\n",
       " 'acquisition': 136,\n",
       " 'acre': 137,\n",
       " 'acreage': 138,\n",
       " 'acres': 139,\n",
       " 'acrobat': 140,\n",
       " 'acronym': 141,\n",
       " 'acrosstheboard': 142,\n",
       " 'act': 143,\n",
       " 'acted': 144,\n",
       " 'acting': 145,\n",
       " 'action': 146,\n",
       " 'actionable': 147,\n",
       " 'activate': 148,\n",
       " 'activated': 149,\n",
       " 'activation': 150,\n",
       " 'active': 151,\n",
       " 'actively': 152,\n",
       " 'actives': 153,\n",
       " 'activism': 154,\n",
       " 'activist': 155,\n",
       " 'activities': 156,\n",
       " 'activity': 157,\n",
       " 'actor': 158,\n",
       " 'actress': 159,\n",
       " 'actual': 160,\n",
       " 'actually': 161,\n",
       " 'acute': 162,\n",
       " 'ada': 163,\n",
       " 'adam': 164,\n",
       " 'adapt': 165,\n",
       " 'adaptation': 166,\n",
       " 'adapted': 167,\n",
       " 'adapter': 168,\n",
       " 'adaptive': 169,\n",
       " 'add': 170,\n",
       " 'adddrop': 171,\n",
       " 'added': 172,\n",
       " 'addedvalue': 173,\n",
       " 'addendum': 174,\n",
       " 'adder': 175,\n",
       " 'addictive': 176,\n",
       " 'addin': 177,\n",
       " 'adding': 178,\n",
       " 'addison': 179,\n",
       " 'addition': 180,\n",
       " 'additional': 181,\n",
       " 'additionally': 182,\n",
       " 'additive': 183,\n",
       " 'addon': 184,\n",
       " 'address': 185,\n",
       " 'addressed': 186,\n",
       " 'addressee': 187,\n",
       " 'addressees': 188,\n",
       " 'addresses': 189,\n",
       " 'addressing': 190,\n",
       " 'addtion': 191,\n",
       " 'addtional': 192,\n",
       " 'adept': 193,\n",
       " 'adequacy': 194,\n",
       " 'adequate': 195,\n",
       " 'adequately': 196,\n",
       " 'ader': 197,\n",
       " 'adhere': 198,\n",
       " 'adherence': 199,\n",
       " 'aditya': 200,\n",
       " 'adjacent': 201,\n",
       " 'adjoining': 202,\n",
       " 'adjourn': 203,\n",
       " 'adjunct': 204,\n",
       " 'adjust': 205,\n",
       " 'adjustable': 206,\n",
       " 'adjusted': 207,\n",
       " 'adjusting': 208,\n",
       " 'adjustment': 209,\n",
       " 'adler': 210,\n",
       " 'admin': 211,\n",
       " 'administered': 212,\n",
       " 'administration': 213,\n",
       " 'administrative': 214,\n",
       " 'administrator': 215,\n",
       " 'admiration': 216,\n",
       " 'admired': 217,\n",
       " 'admirer': 218,\n",
       " 'admission': 219,\n",
       " 'admit': 220,\n",
       " 'admitted': 221,\n",
       " 'admittedly': 222,\n",
       " 'admitting': 223,\n",
       " 'admonished': 224,\n",
       " 'admonition': 225,\n",
       " 'adobe': 226,\n",
       " 'adopt': 227,\n",
       " 'adopted': 228,\n",
       " 'adoption': 229,\n",
       " 'adore': 230,\n",
       " 'adr': 231,\n",
       " 'adrian': 232,\n",
       " 'adtran': 233,\n",
       " 'adult': 234,\n",
       " 'advance': 235,\n",
       " 'advanced': 236,\n",
       " 'advancement': 237,\n",
       " 'advancer': 238,\n",
       " 'advances': 239,\n",
       " 'advancing': 240,\n",
       " 'advantage': 241,\n",
       " 'advantageous': 242,\n",
       " 'advantages': 243,\n",
       " 'advent': 244,\n",
       " 'adventure': 245,\n",
       " 'adventures': 246,\n",
       " 'adverse': 247,\n",
       " 'adversely': 248,\n",
       " 'adversity': 249,\n",
       " 'advertise': 250,\n",
       " 'advertised': 251,\n",
       " 'advertisement': 252,\n",
       " 'advertiser': 253,\n",
       " 'advertises': 254,\n",
       " 'advertising': 255,\n",
       " 'advice': 256,\n",
       " 'advice_____________________': 257,\n",
       " 'advise': 258,\n",
       " 'advised': 259,\n",
       " 'adviser': 260,\n",
       " 'advises': 261,\n",
       " 'advising': 262,\n",
       " 'advisor': 263,\n",
       " 'advisory': 264,\n",
       " 'advocate': 265,\n",
       " 'advocates': 266,\n",
       " 'advocating': 267,\n",
       " 'aec': 268,\n",
       " 'aeco': 269,\n",
       " 'aegis': 270,\n",
       " 'aep': 271,\n",
       " 'aerial': 272,\n",
       " 'aeros': 273,\n",
       " 'aerospace': 274,\n",
       " 'aes': 275,\n",
       " 'aess': 276,\n",
       " 'af56661': 277,\n",
       " 'afar': 278,\n",
       " 'affair': 279,\n",
       " 'affect': 280,\n",
       " 'affected': 281,\n",
       " 'affecting': 282,\n",
       " 'affidavit': 283,\n",
       " 'affiliate': 284,\n",
       " 'affiliated': 285,\n",
       " 'affiliates': 286,\n",
       " 'affiliation': 287,\n",
       " 'affirm': 288,\n",
       " 'affirmation': 289,\n",
       " 'affirmative': 290,\n",
       " 'affirmed': 291,\n",
       " 'afflicted': 292,\n",
       " 'affluent': 293,\n",
       " 'afford': 294,\n",
       " 'affordable': 295,\n",
       " 'afghanistan': 296,\n",
       " 'afl': 297,\n",
       " 'afloat': 298,\n",
       " 'aforementioned': 299,\n",
       " 'afp': 300,\n",
       " 'afpextel': 301,\n",
       " 'afraid': 302,\n",
       " 'africa': 303,\n",
       " 'african': 304,\n",
       " 'afterhour': 305,\n",
       " 'aftermath': 306,\n",
       " 'afternoon': 307,\n",
       " 'afterschool': 308,\n",
       " 'aftertax': 309,\n",
       " 'afx': 310,\n",
       " 'aga': 311,\n",
       " 'against': 312,\n",
       " 'agame': 313,\n",
       " 'agas': 314,\n",
       " 'age': 315,\n",
       " 'aged': 316,\n",
       " 'agencies': 317,\n",
       " 'agency': 318,\n",
       " 'agenda': 319,\n",
       " 'agendas': 320,\n",
       " 'agent': 321,\n",
       " 'ages': 322,\n",
       " 'agg': 323,\n",
       " 'aggie': 324,\n",
       " 'aggies': 325,\n",
       " 'aggravate': 326,\n",
       " 'aggravation': 327,\n",
       " 'aggregate': 328,\n",
       " 'aggregated': 329,\n",
       " 'aggregation': 330,\n",
       " 'aggressive': 331,\n",
       " 'aggressively': 332,\n",
       " 'agile': 333,\n",
       " 'agilent': 334,\n",
       " 'agility': 335,\n",
       " 'aging': 336,\n",
       " 'ago': 337,\n",
       " 'agra': 338,\n",
       " 'agree': 339,\n",
       " 'agreeable': 340,\n",
       " 'agreed': 341,\n",
       " 'agreeing': 342,\n",
       " 'agreement': 343,\n",
       " 'agrees': 344,\n",
       " 'agressive': 345,\n",
       " 'agricultural': 346,\n",
       " 'agriculture': 347,\n",
       " 'agt': 348,\n",
       " 'agua': 349,\n",
       " 'ahanchian': 350,\n",
       " 'ahead': 351,\n",
       " 'ahman': 352,\n",
       " 'ahold': 353,\n",
       " 'ahover': 354,\n",
       " 'aid': 355,\n",
       " 'aide': 356,\n",
       " 'aided': 357,\n",
       " 'aides': 358,\n",
       " 'aig': 359,\n",
       " 'aikman': 360,\n",
       " 'ailing': 361,\n",
       " 'ailment': 362,\n",
       " 'aim': 363,\n",
       " 'aimed': 364,\n",
       " 'aiming': 365,\n",
       " 'ain': 366,\n",
       " 'aint': 367,\n",
       " 'air': 368,\n",
       " 'airborne': 369,\n",
       " 'aircard': 370,\n",
       " 'airconditioning': 371,\n",
       " 'aircraft': 372,\n",
       " 'aired': 373,\n",
       " 'airfare': 374,\n",
       " 'airfares': 375,\n",
       " 'airline': 376,\n",
       " 'airlines': 377,\n",
       " 'airplane': 378,\n",
       " 'airplanes': 379,\n",
       " 'airport': 380,\n",
       " 'airportassessed': 381,\n",
       " 'airquality': 382,\n",
       " 'airtime': 383,\n",
       " 'airway': 384,\n",
       " 'aisle': 385,\n",
       " 'ajay': 386,\n",
       " 'aka': 387,\n",
       " 'akamai': 388,\n",
       " 'aker': 389,\n",
       " 'akili': 390,\n",
       " 'akin': 391,\n",
       " 'alabama': 392,\n",
       " 'aladdin': 393,\n",
       " 'alamo': 394,\n",
       " 'alamos': 395,\n",
       " 'alan': 396,\n",
       " 'alarm': 397,\n",
       " 'alas': 398,\n",
       " 'alaska': 399,\n",
       " 'alaskan': 400,\n",
       " 'alaywan': 401,\n",
       " 'albany': 402,\n",
       " 'albeit': 403,\n",
       " 'albert': 404,\n",
       " 'alberta': 405,\n",
       " 'alberto': 406,\n",
       " 'albright': 407,\n",
       " 'album': 408,\n",
       " 'albuquerque': 409,\n",
       " 'alcatel': 410,\n",
       " 'alcoa': 411,\n",
       " 'alcohol': 412,\n",
       " 'alden': 413,\n",
       " 'alder': 414,\n",
       " 'aldine': 415,\n",
       " 'aldrich': 416,\n",
       " 'alejandro': 417,\n",
       " 'alert': 418,\n",
       " 'alerted': 419,\n",
       " 'alerting': 420,\n",
       " 'alex': 421,\n",
       " 'alexander': 422,\n",
       " 'alexandra': 423,\n",
       " 'alexandria': 424,\n",
       " 'alexis': 425,\n",
       " 'alfalfa': 426,\n",
       " 'alfred': 427,\n",
       " 'alfredo': 428,\n",
       " 'algebraic': 429,\n",
       " 'alhambra': 430,\n",
       " 'ali': 431,\n",
       " 'alice': 432,\n",
       " 'align': 433,\n",
       " 'aligncenter': 434,\n",
       " 'aligned': 435,\n",
       " 'aligning': 436,\n",
       " 'alignleft': 437,\n",
       " 'alignment': 438,\n",
       " 'alike': 439,\n",
       " 'alink': 440,\n",
       " 'alison': 441,\n",
       " 'alive': 442,\n",
       " 'allamerica': 443,\n",
       " 'allamerican': 444,\n",
       " 'allan': 445,\n",
       " 'allay': 446,\n",
       " 'allegation': 447,\n",
       " 'allege': 448,\n",
       " 'alleged': 449,\n",
       " 'allegedly': 450,\n",
       " 'alleges': 451,\n",
       " 'allegheny': 452,\n",
       " 'allegiance': 453,\n",
       " 'alleging': 454,\n",
       " 'allen': 455,\n",
       " 'allenhouect': 456,\n",
       " 'alleviate': 457,\n",
       " 'alley': 458,\n",
       " 'alliance': 459,\n",
       " 'alliances': 460,\n",
       " 'allies': 461,\n",
       " 'allin': 462,\n",
       " 'allinclusive': 463,\n",
       " 'allinone': 464,\n",
       " 'allison': 465,\n",
       " 'allnew': 466,\n",
       " 'allocate': 467,\n",
       " 'allocated': 468,\n",
       " 'allocates': 469,\n",
       " 'allocation': 470,\n",
       " 'allot': 471,\n",
       " 'allotment': 472,\n",
       " 'allotted': 473,\n",
       " 'allowable': 474,\n",
       " 'allowance': 475,\n",
       " 'allowances': 476,\n",
       " 'allowed': 477,\n",
       " 'allowing': 478,\n",
       " 'alloy': 479,\n",
       " 'allpurpose': 480,\n",
       " 'allred': 481,\n",
       " 'allstar': 482,\n",
       " 'allstock': 483,\n",
       " 'alltime': 484,\n",
       " 'alluded': 485,\n",
       " 'alluring': 486,\n",
       " 'alma': 487,\n",
       " 'almeida': 488,\n",
       " 'almond': 489,\n",
       " 'aloha': 490,\n",
       " 'alongside': 491,\n",
       " 'alot': 492,\n",
       " 'aloud': 493,\n",
       " 'alpert': 494,\n",
       " 'alphanumeric': 495,\n",
       " 'alpharetta': 496,\n",
       " 'alqaeda': 497,\n",
       " 'alright': 498,\n",
       " 'alstott': 499,\n",
       " 'alt': 500,\n",
       " 'altar': 501,\n",
       " 'altec': 502,\n",
       " 'alter': 503,\n",
       " 'alteration': 504,\n",
       " 'altered': 505,\n",
       " 'alternate': 506,\n",
       " 'alternates': 507,\n",
       " 'alternating': 508,\n",
       " 'alternative': 509,\n",
       " 'alternativeenergy': 510,\n",
       " 'alternatively': 511,\n",
       " 'alternatives': 512,\n",
       " 'alto': 513,\n",
       " 'altogether': 514,\n",
       " 'altra': 515,\n",
       " 'alum': 516,\n",
       " 'aluminum': 517,\n",
       " 'alumni': 518,\n",
       " 'alvarez': 519,\n",
       " 'alvin': 520,\n",
       " 'alyson': 521,\n",
       " 'alzheimer': 522,\n",
       " 'amabile': 523,\n",
       " 'amalgamated': 524,\n",
       " 'amanda': 525,\n",
       " 'amani': 526,\n",
       " 'amar': 527,\n",
       " 'amarillo': 528,\n",
       " 'amassed': 529,\n",
       " 'amateur': 530,\n",
       " 'amaze': 531,\n",
       " 'amazed': 532,\n",
       " 'amazing': 533,\n",
       " 'amazingly': 534,\n",
       " 'amazon': 535,\n",
       " 'amazoncom': 536,\n",
       " 'ambassador': 537,\n",
       " 'ambiguity': 538,\n",
       " 'ambition': 539,\n",
       " 'ambitious': 540,\n",
       " 'ambler': 541,\n",
       " 'ambulance': 542,\n",
       " 'amc': 543,\n",
       " 'amen': 544,\n",
       " 'amend': 545,\n",
       " 'amended': 546,\n",
       " 'amending': 547,\n",
       " 'amendment': 548,\n",
       " 'amenities': 549,\n",
       " 'amerada': 550,\n",
       " 'ameren': 551,\n",
       " 'amerex': 552,\n",
       " 'america': 553,\n",
       " 'america_corp': 554,\n",
       " 'american': 555,\n",
       " 'americas': 556,\n",
       " 'ameritrade': 557,\n",
       " 'amex': 558,\n",
       " 'amgen': 559,\n",
       " 'ami': 560,\n",
       " 'amid': 561,\n",
       " 'amidst': 562,\n",
       " 'amie': 563,\n",
       " 'amir': 564,\n",
       " 'ammunition': 565,\n",
       " 'amnesty': 566,\n",
       " 'amoco': 567,\n",
       " 'amortization': 568,\n",
       " 'amortize': 569,\n",
       " 'amortized': 570,\n",
       " 'amos': 571,\n",
       " 'amount': 572,\n",
       " 'amounted': 573,\n",
       " 'amp': 574,\n",
       " 'ampersand': 575,\n",
       " 'ample': 576,\n",
       " 'amplified': 577,\n",
       " 'amplifier': 578,\n",
       " 'amr': 579,\n",
       " 'amro': 580,\n",
       " 'amsterdam': 581,\n",
       " 'amt': 582,\n",
       " 'amusing': 583,\n",
       " 'amy': 584,\n",
       " 'anadarko': 585,\n",
       " 'anaheim': 586,\n",
       " 'analog': 587,\n",
       " 'analogous': 588,\n",
       " 'analogy': 589,\n",
       " 'analyses': 590,\n",
       " 'analysis': 591,\n",
       " 'analysis_____________________': 592,\n",
       " 'analyst': 593,\n",
       " 'analystassociate': 594,\n",
       " 'analytic': 595,\n",
       " 'analytical': 596,\n",
       " 'analyze': 597,\n",
       " 'analyzed': 598,\n",
       " 'analyzer': 599,\n",
       " 'analyzes': 600,\n",
       " 'analyzing': 601,\n",
       " 'anatol': 602,\n",
       " 'anchor': 603,\n",
       " 'anchordesk': 604,\n",
       " 'anchored': 605,\n",
       " 'ancient': 606,\n",
       " 'ancillary': 607,\n",
       " 'ander': 608,\n",
       " 'andersen': 609,\n",
       " 'anderson': 610,\n",
       " 'andor': 611,\n",
       " 'andover': 612,\n",
       " 'andre': 613,\n",
       " 'andrea': 614,\n",
       " 'andrew': 615,\n",
       " 'andy': 616,\n",
       " 'anecdotal': 617,\n",
       " 'anecdotes': 618,\n",
       " 'anemia': 619,\n",
       " 'anemic': 620,\n",
       " 'anew': 621,\n",
       " 'angel': 622,\n",
       " 'angela': 623,\n",
       " 'angelides': 624,\n",
       " 'angelo': 625,\n",
       " 'angelos': 626,\n",
       " 'anger': 627,\n",
       " 'angered': 628,\n",
       " 'angie': 629,\n",
       " 'angle': 630,\n",
       " 'angles': 631,\n",
       " 'angry': 632,\n",
       " 'angst': 633,\n",
       " 'animal': 634,\n",
       " 'animated': 635,\n",
       " 'animation': 636,\n",
       " 'animosity': 637,\n",
       " 'ankle': 638,\n",
       " 'ankles': 639,\n",
       " 'ann': 640,\n",
       " 'anna': 641,\n",
       " 'annapolis': 642,\n",
       " 'anne': 643,\n",
       " 'annex': 644,\n",
       " 'annexes': 645,\n",
       " 'annie': 646,\n",
       " 'anniversaries': 647,\n",
       " 'anniversary': 648,\n",
       " 'announce': 649,\n",
       " 'announced': 650,\n",
       " 'announcement': 651,\n",
       " 'announcer': 652,\n",
       " 'announces': 653,\n",
       " 'announcing': 654,\n",
       " 'annoyance': 655,\n",
       " 'annoying': 656,\n",
       " 'annual': 657,\n",
       " 'annualized': 658,\n",
       " 'annually': 659,\n",
       " 'annuities': 660,\n",
       " 'annuity': 661,\n",
       " 'anointed': 662,\n",
       " 'anomaly': 663,\n",
       " 'anonymity': 664,\n",
       " 'anonymous': 665,\n",
       " 'anp': 666,\n",
       " 'answer': 667,\n",
       " 'answerable': 668,\n",
       " 'answered': 669,\n",
       " 'answering': 670,\n",
       " 'ant': 671,\n",
       " 'ante': 672,\n",
       " 'anthem': 673,\n",
       " 'anthony': 674,\n",
       " 'anthrax': 675,\n",
       " 'anti': 676,\n",
       " 'anticipate': 677,\n",
       " 'anticipated': 678,\n",
       " 'anticipates': 679,\n",
       " 'anticipating': 680,\n",
       " 'anticipation': 681,\n",
       " 'antigua': 682,\n",
       " 'antioch': 683,\n",
       " 'antique': 684,\n",
       " 'antiques': 685,\n",
       " 'antiterror': 686,\n",
       " 'antiterrorism': 687,\n",
       " 'antiterrorist': 688,\n",
       " 'antitrust': 689,\n",
       " 'antivirus': 690,\n",
       " 'anton': 691,\n",
       " 'antonio': 692,\n",
       " 'antwan': 693,\n",
       " 'anxiety': 694,\n",
       " 'anxious': 695,\n",
       " 'anymore': 696,\n",
       " 'anyones': 697,\n",
       " 'anytime': 698,\n",
       " 'anyway': 699,\n",
       " 'anz': 700,\n",
       " 'aol': 701,\n",
       " 'aon': 702,\n",
       " 'apa': 703,\n",
       " 'apache': 704,\n",
       " 'apartment': 705,\n",
       " 'apb': 706,\n",
       " 'apex': 707,\n",
       " 'api': 708,\n",
       " 'apiece': 709,\n",
       " 'apollo': 710,\n",
       " 'apologies': 711,\n",
       " 'apologist': 712,\n",
       " 'apologize': 713,\n",
       " 'apologized': 714,\n",
       " 'apology': 715,\n",
       " 'app': 716,\n",
       " 'appalachian': 717,\n",
       " 'apparel': 718,\n",
       " 'apparent': 719,\n",
       " 'apparently': 720,\n",
       " 'appeal': 721,\n",
       " 'appealed': 722,\n",
       " 'appealing': 723,\n",
       " 'appearance': 724,\n",
       " 'appearances': 725,\n",
       " 'appeared': 726,\n",
       " 'appearing': 727,\n",
       " 'appease': 728,\n",
       " 'appellate': 729,\n",
       " 'appendices': 730,\n",
       " 'appendix': 731,\n",
       " 'appetite': 732,\n",
       " 'appetizer': 733,\n",
       " 'applaud': 734,\n",
       " 'applauded': 735,\n",
       " 'applause': 736,\n",
       " 'apple': 737,\n",
       " 'applebaum': 738,\n",
       " 'apples': 739,\n",
       " 'appleton': 740,\n",
       " 'applewhite': 741,\n",
       " 'applewhites': 742,\n",
       " 'appliance': 743,\n",
       " 'appliances': 744,\n",
       " 'applicability': 745,\n",
       " 'applicable': 746,\n",
       " 'applicant': 747,\n",
       " 'application': 748,\n",
       " 'applied': 749,\n",
       " 'applies': 750,\n",
       " 'apply': 751,\n",
       " 'applying': 752,\n",
       " 'appoint': 753,\n",
       " 'appointed': 754,\n",
       " 'appointee': 755,\n",
       " 'appointment': 756,\n",
       " 'appraisal': 757,\n",
       " 'appraised': 758,\n",
       " 'appreciate': 759,\n",
       " 'appreciated': 760,\n",
       " 'appreciates': 761,\n",
       " 'appreciation': 762,\n",
       " 'approach': 763,\n",
       " 'approached': 764,\n",
       " 'approaches': 765,\n",
       " 'approaching': 766,\n",
       " 'appropriately': 767,\n",
       " 'appropriation': 768,\n",
       " 'approval': 769,\n",
       " 'approve': 770,\n",
       " 'approved': 771,\n",
       " 'approver': 772,\n",
       " 'approves': 773,\n",
       " 'approving': 774,\n",
       " 'approx': 775,\n",
       " 'approximate': 776,\n",
       " 'approximately': 777,\n",
       " 'appt': 778,\n",
       " 'apr': 779,\n",
       " 'apr01': 780,\n",
       " 'april': 781,\n",
       " 'apses': 782,\n",
       " 'apt': 783,\n",
       " 'apx': 784,\n",
       " 'aquila': 785,\n",
       " 'arab': 786,\n",
       " 'aramco': 787,\n",
       " 'arb': 788,\n",
       " 'arbiter': 789,\n",
       " 'arbitrage': 790,\n",
       " 'arbitrageur': 791,\n",
       " 'arbitrarily': 792,\n",
       " 'arbitration': 793,\n",
       " 'arbitrator': 794,\n",
       " 'arbor': 795,\n",
       " 'arc': 796,\n",
       " 'arcade': 797,\n",
       " 'archer': 798,\n",
       " 'architect': 799,\n",
       " 'architectural': 800,\n",
       " 'architecture': 801,\n",
       " 'archival': 802,\n",
       " 'archive': 803,\n",
       " 'archived': 804,\n",
       " 'archives': 805,\n",
       " 'area': 806,\n",
       " 'areas': 807,\n",
       " 'arena': 808,\n",
       " 'arent': 809,\n",
       " 'argentina': 810,\n",
       " 'argentine': 811,\n",
       " 'arguably': 812,\n",
       " 'argue': 813,\n",
       " 'argued': 814,\n",
       " 'argues': 815,\n",
       " 'arguing': 816,\n",
       " 'argument': 817,\n",
       " 'argus': 818,\n",
       " 'ari': 819,\n",
       " 'aria': 820,\n",
       " 'arial': 821,\n",
       " 'arise': 822,\n",
       " 'arisen': 823,\n",
       " 'arises': 824,\n",
       " 'arising': 825,\n",
       " 'ariz': 826,\n",
       " 'arizona': 827,\n",
       " 'arizonas': 828,\n",
       " 'ark': 829,\n",
       " 'arkansas': 830,\n",
       " 'arlington': 831,\n",
       " 'arm': 832,\n",
       " 'armando': 833,\n",
       " 'armed': 834,\n",
       " 'armen': 835,\n",
       " 'armies': 836,\n",
       " 'armstrong': 837,\n",
       " 'army': 838,\n",
       " 'arnold': 839,\n",
       " 'arnoldougcooenron': 840,\n",
       " 'aromas': 841,\n",
       " 'aromatic': 842,\n",
       " 'aron': 843,\n",
       " 'arora': 844,\n",
       " 'arose': 845,\n",
       " 'aroundtheclock': 846,\n",
       " 'aroused': 847,\n",
       " 'arpt': 848,\n",
       " 'arrange': 849,\n",
       " 'arranged': 850,\n",
       " 'arrangement': 851,\n",
       " 'arranging': 852,\n",
       " 'arrangment': 853,\n",
       " 'array': 854,\n",
       " 'arredondo': 855,\n",
       " 'arrest': 856,\n",
       " 'arrested': 857,\n",
       " 'arris': 858,\n",
       " 'arrival': 859,\n",
       " 'arrive': 860,\n",
       " 'arrived': 861,\n",
       " 'arrives': 862,\n",
       " 'arriving': 863,\n",
       " 'arrogance': 864,\n",
       " 'arrogant': 865,\n",
       " 'arrow': 866,\n",
       " 'arroyo': 867,\n",
       " 'arsenal': 868,\n",
       " 'arslanian': 869,\n",
       " 'art': 870,\n",
       " 'arter': 871,\n",
       " 'artesa': 872,\n",
       " 'arthritis': 873,\n",
       " 'arthroscopic': 874,\n",
       " 'arthur': 875,\n",
       " 'article': 876,\n",
       " 'articles': 877,\n",
       " 'articulate': 878,\n",
       " 'articulated': 879,\n",
       " 'artifact': 880,\n",
       " 'artificial': 881,\n",
       " 'artificially': 882,\n",
       " 'artist': 883,\n",
       " 'artwork': 884,\n",
       " 'as400': 885,\n",
       " 'asa': 886,\n",
       " 'asap': 887,\n",
       " 'asc': 888,\n",
       " 'ascend': 889,\n",
       " 'ascension': 890,\n",
       " 'ascent': 891,\n",
       " 'ascertain': 892,\n",
       " 'aschehoug': 893,\n",
       " 'asem': 894,\n",
       " 'ash': 895,\n",
       " 'ashamed': 896,\n",
       " 'ashcroft': 897,\n",
       " 'ashford': 898,\n",
       " 'ashland': 899,\n",
       " 'ashley': 900,\n",
       " 'ashore': 901,\n",
       " 'ashton': 902,\n",
       " 'asia': 903,\n",
       " 'asian': 904,\n",
       " 'asiapacific': 905,\n",
       " 'asias': 906,\n",
       " 'ask': 907,\n",
       " 'asked': 908,\n",
       " 'asking': 909,\n",
       " 'asleep': 910,\n",
       " 'asp': 911,\n",
       " 'aspect': 912,\n",
       " 'asphalt': 913,\n",
       " 'aspiration': 914,\n",
       " 'ass': 915,\n",
       " 'assailed': 916,\n",
       " 'assault': 917,\n",
       " 'assemble': 918,\n",
       " 'assembled': 919,\n",
       " 'assembly': 920,\n",
       " 'assemblywoman': 921,\n",
       " 'assert': 922,\n",
       " 'asserted': 923,\n",
       " 'asserting': 924,\n",
       " 'assertion': 925,\n",
       " 'assertive': 926,\n",
       " 'asses': 927,\n",
       " 'assess': 928,\n",
       " 'assessed': 929,\n",
       " 'assesses': 930,\n",
       " 'assessing': 931,\n",
       " 'assessment': 932,\n",
       " 'asset': 933,\n",
       " 'assetbased': 934,\n",
       " 'asshole': 935,\n",
       " 'assign': 936,\n",
       " 'assigned': 937,\n",
       " 'assignment': 938,\n",
       " 'assist': 939,\n",
       " 'assistance': 940,\n",
       " 'assistant': 941,\n",
       " 'assisted': 942,\n",
       " 'assisting': 943,\n",
       " 'assoc': 944,\n",
       " 'associate': 945,\n",
       " 'associates': 946,\n",
       " 'association': 947,\n",
       " 'assorted': 948,\n",
       " 'asst': 949,\n",
       " 'assuage': 950,\n",
       " 'assume': 951,\n",
       " 'assumed': 952,\n",
       " 'assumes': 953,\n",
       " 'assuming': 954,\n",
       " 'assumption': 955,\n",
       " 'assurance': 956,\n",
       " 'assurances': 957,\n",
       " 'assure': 958,\n",
       " 'assured': 959,\n",
       " 'assuring': 960,\n",
       " 'astoria': 961,\n",
       " 'astounding': 962,\n",
       " 'astray': 963,\n",
       " 'astrodome': 964,\n",
       " 'astronomical': 965,\n",
       " 'astros': 966,\n",
       " 'asylum': 967,\n",
       " 'ata': 968,\n",
       " 'atc': 969,\n",
       " 'ate': 970,\n",
       " 'athe': 971,\n",
       " 'atheist': 972,\n",
       " 'athen': 973,\n",
       " 'athlete': 974,\n",
       " 'athletes': 975,\n",
       " 'athletic': 976,\n",
       " 'athleticism': 977,\n",
       " 'atkin': 978,\n",
       " 'atl': 979,\n",
       " 'atlanta': 980,\n",
       " 'atlantabased': 981,\n",
       " 'atlantas': 982,\n",
       " 'atlantic': 983,\n",
       " 'atlantis': 984,\n",
       " 'atlas': 985,\n",
       " 'atm': 986,\n",
       " 'atmosphere': 987,\n",
       " 'atmospheric': 988,\n",
       " 'atom': 989,\n",
       " 'atop': 990,\n",
       " 'att': 991,\n",
       " 'att1htm': 992,\n",
       " 'atta': 993,\n",
       " 'attach': 994,\n",
       " 'attached': 995,\n",
       " 'attaching': 996,\n",
       " 'attack': 997,\n",
       " 'attacked': 998,\n",
       " 'attacking': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1152 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each of the topics synthesized, get the word distribution\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #Use softmax function to assign probability for each of the word associated with the topic\n",
    "    # The sum of all the probabilities of words associated with each topic should add up to 1\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "    #print(\"Printing pvals: \", len(pvals))\n",
    "    \n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
